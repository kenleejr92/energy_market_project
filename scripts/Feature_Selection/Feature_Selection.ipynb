{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenleejr92/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:26: DeprecationWarning: the sets module is deprecated\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import MySQLdb\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import holidays\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.insert(0, '/Users/kenleejr92/energy_market_project/scripts/MySQL_scripts')\n",
    "from Query_ERCOT_DB import Query_ERCOT_DB\n",
    "import cPickle as pickle\n",
    "from datetime import datetime\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.linalg import Matrix, Matrices\n",
    "import pyspark.mllib.linalg.distributed as pydist\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "import mpld3\n",
    "import re\n",
    "from sets import Set\n",
    "mpld3.enable_notebook()\n",
    "\n",
    "class LMP_Query(Query_ERCOT_DB):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.SPPs = ['HB_BUSAVG', \n",
    "                     'HB_HOUSTON', \n",
    "                     'HB_HUBAVG', \n",
    "                     'HB_NORTH', \n",
    "                     'HB_SOUTH', \n",
    "                     'HB_WEST', \n",
    "                     'LZ_AEN', \n",
    "                     'LZ_CPS', \n",
    "                     'LZ_HOUSTON', \n",
    "                     'LZ_LCRA', \n",
    "                     'LZ_NORTH', \n",
    "                     'LZ_RAYBN', \n",
    "                     'LZ_SOUTH',  \n",
    "                     'LZ_WEST']   \n",
    "        self.node_dict = {}\n",
    "        self.table_list = []\n",
    "        self.df = None\n",
    "        self.CRR_nodes = None\n",
    "        self.table_boundaries = {'table0':('0001', 'BLUEMD1_8X'),\n",
    "                                 'table1':('BLUEMD1_8Z', 'CHT_M'),\n",
    "                                 'table2':('CHT_X', 'DUKE_8405'),\n",
    "                                 'table3':('DUKE_8505', 'ELEVEE_E8'),\n",
    "                                 'table4':('ELEVEE_W8', 'GREENLK_L_A'),\n",
    "                                 'table5':('GREENLK_L_B', 'KEETER'),\n",
    "                                 'table6':('KEITH', 'L_CEDAHI8_1Y'),\n",
    "                                 'table7':('L_CEDAHI8_1Z', 'MOSES_1G'),\n",
    "                                 'table8':('MOSES_2G', 'PHR_8135'),\n",
    "                                 'table9':('PHR_8140', 'SANDOW1_8Y'),\n",
    "                                 'table10':('SANDOW_4G', 'TCN7225_BUS'),\n",
    "                                 'table11': ('TCN7230_BUS', 'VENSW_1777'),\n",
    "                                 'table12':('VENSW_1785', '_WC_V_C')\n",
    "                                 }\n",
    "        for i in range(0,13):\n",
    "            Query_ERCOT_DB.c.execute(\"\"\"SHOW columns FROM DAM_LMP%s\"\"\" % i)\n",
    "            result = [r[0] for r in Query_ERCOT_DB.c.fetchall()[2:]]\n",
    "            self.table_list.append(result)\n",
    "            for node in result:\n",
    "                self.node_dict[node] = i\n",
    "        \n",
    "    \n",
    "    def get_CRR_nodes(self):\n",
    "        Query_ERCOT_DB.c.execute(\"\"\"SELECT DISTINCT Sink FROM crr_ownership ORDER BY Sink\"\"\")\n",
    "        nodes = list(Query_ERCOT_DB.c.fetchall())\n",
    "        CRR_prefixes = [r[0] for r in nodes]\n",
    "        pattern = re.compile('.*_')\n",
    "        matching_patterns = Set()\n",
    "        for idx, node in enumerate(CRR_prefixes):\n",
    "            matches = re.findall(pattern, node)\n",
    "            if matches: matching_patterns.add(matches[0][:-1])\n",
    "        # flatten list of lists      \n",
    "        all_nodes = [item for sublist in self.table_list for item in sublist] + self.SPPs\n",
    "        self.CRR_nodes = []\n",
    "        for pattern in matching_patterns:\n",
    "            pattern2 = re.compile('(.*%s.*)' % pattern)\n",
    "            for node in all_nodes:\n",
    "                if re.search(pattern2, node):\n",
    "                    self.CRR_nodes.append(node)\n",
    "        return self.CRR_nodes\n",
    "    \n",
    "    def query_single_node(self, node):\n",
    "        s=\"\"\"SELECT delivery_date, hour_ending, %s from DAM_LMP%s order by delivery_date, hour_ending\"\"\" % (node, self.node_dict[node])\n",
    "        Query_ERCOT_DB.c.execute(s)\n",
    "        result = list(Query_ERCOT_DB.c.fetchall())\n",
    "        fresult = []\n",
    "        for r in result:\n",
    "            temp = ()\n",
    "            date = r[0]\n",
    "            time = str(int(r[1].split(\":\")[0])-1)\n",
    "            dt = datetime.strptime(date + \" \" + time, \"%Y-%m-%d %H\")\n",
    "            for x in r[2:]:\n",
    "                if x == None: x = 0\n",
    "                temp = temp + (float(x),)\n",
    "            r = (dt,) + temp\n",
    "            fresult.append(r)\n",
    "        self.df = pd.DataFrame(data=[f[1:] for f in fresult], index=[r[0] for r in fresult], columns=[node])\n",
    "        return self.df\n",
    "        \n",
    "    def query(self, nodes, start_date='2011-01-01', end_date='2015-12-31'):\n",
    "        node_string=''\n",
    "        for node in nodes:\n",
    "            node_string = node_string + node + ',' + ' '\n",
    "        node_string = node_string[:-2]\n",
    "        s = \"\"\"SELECT DAM_LMP0.delivery_date, DAM_LMP0.hour_ending, %s \n",
    "                                    from DAM_LMP0 join DAM_LMP1 on (DAM_LMP0.delivery_date = DAM_LMP1.delivery_date and DAM_LMP0.hour_ending = DAM_LMP1.hour_ending) \n",
    "                                    join DAM_LMP2 on (DAM_LMP0.delivery_date = DAM_LMP2.delivery_date and DAM_LMP0.hour_ending = DAM_LMP2.hour_ending)\n",
    "                                    join DAM_LMP3 on (DAM_LMP0.delivery_date = DAM_LMP3.delivery_date and DAM_LMP0.hour_ending = DAM_LMP3.hour_ending)\n",
    "                                    join DAM_LMP4 on (DAM_LMP0.delivery_date = DAM_LMP4.delivery_date and DAM_LMP0.hour_ending = DAM_LMP4.hour_ending)\n",
    "                                    join DAM_LMP5 on (DAM_LMP0.delivery_date = DAM_LMP5.delivery_date and DAM_LMP0.hour_ending = DAM_LMP5.hour_ending)\n",
    "                                    join DAM_LMP6 on (DAM_LMP0.delivery_date = DAM_LMP6.delivery_date and DAM_LMP0.hour_ending = DAM_LMP6.hour_ending)\n",
    "                                    join DAM_LMP7 on (DAM_LMP0.delivery_date = DAM_LMP7.delivery_date and DAM_LMP0.hour_ending = DAM_LMP7.hour_ending)\n",
    "                                    join DAM_LMP8 on (DAM_LMP0.delivery_date = DAM_LMP8.delivery_date and DAM_LMP0.hour_ending = DAM_LMP8.hour_ending)\n",
    "                                    join DAM_LMP9 on (DAM_LMP0.delivery_date = DAM_LMP9.delivery_date and DAM_LMP0.hour_ending = DAM_LMP9.hour_ending)\n",
    "                                    join DAM_LMP10 on (DAM_LMP0.delivery_date = DAM_LMP10.delivery_date and DAM_LMP0.hour_ending = DAM_LMP10.hour_ending)\n",
    "                                    join DAM_LMP11 on (DAM_LMP0.delivery_date = DAM_LMP11.delivery_date and DAM_LMP0.hour_ending = DAM_LMP11.hour_ending)\n",
    "                                    join DAM_LMP12 on (DAM_LMP0.delivery_date = DAM_LMP12.delivery_date and DAM_LMP0.hour_ending = DAM_LMP12.hour_ending)\n",
    "                                    join DAM_SPPs on (DAM_LMP0.delivery_date = DAM_SPPs.delivery_date and DAM_LMP0.hour_ending = DAM_SPPs.hour_ending)\n",
    "                                    where DAM_LMP0.delivery_date > \"%s\" and DAM_LMP0.delivery_date < \"%s\" order by DAM_LMP0.delivery_date, DAM_LMP0.hour_ending;\"\"\" % (node_string, start_date, end_date)\n",
    "        Query_ERCOT_DB.c.execute(s)\n",
    "        result = list(Query_ERCOT_DB.c.fetchall())\n",
    "        fresult = []\n",
    "        for r in result:\n",
    "            temp = ()\n",
    "            date = r[0]\n",
    "            time = str(int(r[1].split(\":\")[0])-1)\n",
    "            dt = datetime.strptime(date + \" \" + time, \"%Y-%m-%d %H\")\n",
    "            for x in r[2:]:\n",
    "                if x == None: x = 0\n",
    "                temp = temp + (float(x),)\n",
    "            r = (dt,) + temp\n",
    "            fresult.append(r)\n",
    "        self.df = pd.DataFrame(data=[f[1:] for f in fresult], index=[r[0] for r in fresult], columns=nodes)\n",
    "        return self.df\n",
    "        \n",
    "    def get_price(self, node, date, hour_ending):\n",
    "        for i in range(0,13):\n",
    "            node = append_n(node)\n",
    "            if node in self.table_columns['table%s' % i]:\n",
    "                Query_ERCOT_DB.c.execute(\"\"\"SELECT %s FROM DAM_LMP%s WHERE delivery_date = \"%s\" AND hour_ending = \\\"%s\\\"\"\"\" % (node, i, date, hour_ending))\n",
    "                result = list(Query_ERCOT_DB.c.fetchall())[0][0]\n",
    "                return result\n",
    "        \n",
    "\n",
    "def append_n(name):\n",
    "    if name[0] in ['0','1','2','3','4','5','6','7','8','9'] or name == 'LOAD':\n",
    "        name = 'n' + name\n",
    "    return name\n",
    "\n",
    "def dist(x,y):\n",
    "    cost = 0\n",
    "    for i in range(len(x)):\n",
    "        if x[i] == 0 or y[i] == 0:\n",
    "            continue\n",
    "        else:\n",
    "            cost = cost + np.abs(y[i] - x[i])[0]\n",
    "    return cost\n",
    "\n",
    "def string_to_date(string_date):\n",
    "    return datetime.strptime(string_date, \"%Y-%m-%d %H\")\n",
    "\n",
    "def date_to_string(date):\n",
    "    return date.strftime(\"%Y-%m-%d %H\")\n",
    "\n",
    "def weekday_of_date(date):\n",
    "    return calendar.day_name[date.weekday()]\n",
    "\n",
    "def work_day_or_holiday(date):\n",
    "    us_holidays = holidays.UnitedStates()\n",
    "    if date in us_holidays or weekday_of_date(date) == \"Sunday\" or weekday_of_date(date) == \"Saturday\":\n",
    "        return int(1)\n",
    "    else: return int(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"spark_DAM_correlation\")\n",
    "sc = SparkContext(conf = conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qcrr = LMP_Query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crr_nodes = qcrr.get_CRR_nodes()\n",
    "f = open('crr_nodes.pkl', 'w+')\n",
    "pickle.dump(crr_nodes, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1111)\n",
    "f2 = open('crr_nodes.pkl', 'r')\n",
    "crr_nodes = pickle.load(f2)\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1 = qcrr.query(crr_nodes[0:500],'2011-01-01','2015-12-31').as_matrix()\n",
    "x2 = qcrr.query(crr_nodes[500:1000], '2011-01-01','2015-12-31').as_matrix()\n",
    "x3 = qcrr.query(crr_nodes[1000:1500], '2011-01-01','2015-12-31').as_matrix()\n",
    "x4 = qcrr.query(crr_nodes[1500:2000], '2011-01-01','2015-12-31').as_matrix()\n",
    "x5 = qcrr.query(crr_nodes[2000:2500], '2011-01-01','2015-12-31').as_matrix()\n",
    "x6 = qcrr.query(crr_nodes[2500:3000], '2011-01-01','2015-12-31').as_matrix()\n",
    "x7 = qcrr.query(crr_nodes[3000:], '2011-01-01','2015-12-31').as_matrix()\n",
    "x8 = np.concatenate((x1,x2), axis=1)\n",
    "x8 = np.concatenate((x8,x3), axis=1)\n",
    "x8 = np.concatenate((x8,x4), axis=1)\n",
    "x8 = np.concatenate((x8,x5), axis=1)\n",
    "x8 = np.concatenate((x8,x6), axis=1)\n",
    "x8 = np.concatenate((x8,x7), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1 = open('crr_node_prices.csv', 'w+')\n",
    "np.savetxt('crr_node_prices.csv', x8, delimiter=',', fmt='%10.5f')\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3588"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_df = pd.DataFrame.from_csv('/Users/kenleejr92/energy_market_project/crr_node_correlation_matrix.csv', header=None, index_col=None)\n",
    "x = x_df.as_matrix()\n",
    "for i, item in enumerate(x[:,0]):\n",
    "    x[i,0] = float(item[1:])\n",
    "for i, item in enumerate(x[:,3587]):\n",
    "    x[i,3587] = float(item[:-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correlation_matrix = np.abs(x)\n",
    "af = AffinityPropagation(affinity='precomputed').fit(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_dict = {}\n",
    "for i, col in enumerate(crr_nodes):\n",
    "    col_dict[i] = col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_list = [[] for i in range(af.cluster_centers_indices_.size)]\n",
    "for idx, item in enumerate(af.labels_):\n",
    "    cluster_list[item].append(col_dict[idx])\n",
    "exemplars = []\n",
    "for idx, cluster in enumerate(cluster_list):\n",
    "    if len(cluster)>=5:\n",
    "        exemplars.append(col_dict[idx])\n",
    "f = open('cluster_list.pkl', 'w+')\n",
    "pickle.dump(cluster_list, f)\n",
    "f.close()\n",
    "f = open('exemplars.pkl', 'w+')\n",
    "pickle.dump(exemplars, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('exemplars.pkl', 'r')\n",
    "exemplars = pickle.load(f)\n",
    "f.close()\n",
    "df = qcrr.query(exemplars)\n",
    "ax = df.plot()\n",
    "ax.set_xlabel('Hour')\n",
    "ax.set_ylabel('Dollar/MWh')\n",
    "ax.legend_.remove()\n",
    "plt.show()\n",
    "f = open('exemplar_dataframe.pkl', 'w+')\n",
    "pickle.dump(df, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature selection for NCARBIDE_E_2\n",
      "NCARBIDE_E_2\n",
      "NCARBIDE_E_3\n",
      "NCARBIDE_E_4\n",
      "BL_MILIT_001\n",
      "BL_MILIT_BLT\n",
      "BL_MILIT_L_A\n",
      "BRYN_MILLICZ\n",
      "EBWINDMILL28\n",
      "EB_MILTON\n",
      "HAMILMB1X\n",
      "HAMILMB1Y\n",
      "HAMILMB1Z\n",
      "HAMILMB2\n",
      "HAMILTON\n",
      "HAMILTON_L_B\n",
      "HAMILTON_V_A\n",
      "HAMILTON_V_B\n",
      "HAMILTO_0001\n",
      "MILDRED2_9\n",
      "MILDRED2_9X\n",
      "MILE_L_A\n",
      "MILE_V_A\n",
      "MILITARY_V_A\n",
      "MILITARY_V_B\n",
      "MILLCR_L_A\n",
      "MILO_L_A\n",
      "RWMILLER\n",
      "RWMILLER1\n",
      "RWMILLER2\n",
      "TNHAMILTON0Y\n",
      "QALSW_0001\n",
      "QALSW_0100\n",
      "QALSW_AG3\n",
      "QALSW_GT2\n",
      "EXNMEANS_8Z\n",
      "EXN_69A\n",
      "EXN_EPS_1\n",
      "EXN_EPS_2\n",
      "EXN_G10\n",
      "EXN_G12\n",
      "EXN_G13\n",
      "EXN_G14\n",
      "LNCRK2_871_2\n",
      "HLSES_8620\n",
      "HLSES_8621\n",
      "HLSES_8637\n",
      "HLSES_KL\n",
      "HLSES_KS\n",
      "HLSES_KY\n",
      "OGSES_K\n",
      "OGSES_KE\n",
      "OGSES_KH\n",
      "SWEC_0001\n",
      "GUADG_GAS1\n",
      "GUADG_GAS3\n",
      "GUADG_GAS4\n",
      "GUADG_STM5\n",
      "GUADG_STM6\n",
      "GUADG_V_A\n",
      "WHCCS_E_1\n",
      "WHCCS_E_2\n",
      "R_STEAM_2\n",
      "STEAM1A_0004\n",
      "STEAM_A901\n",
      "STEAM_KTK\n",
      "STEAM_LD04\n",
      "DIB_DIB_G1\n",
      "DIB_DIB_G2\n",
      "RSKMN_KU\n",
      "FORSAN_8\n",
      "L_SANDHI8_1Y\n",
      "L_SANMAR8_1X\n",
      "L_SANMAR8_1Z\n",
      "L_SANSAS9_1Z\n",
      "MONSAN__138R\n",
      "ODESSANO_8S\n",
      "ODESSANO_8T\n",
      "PLEASANTON9X\n",
      "PLEASANT_L_B\n",
      "PLEASANT_MOB\n",
      "PLEASANT_V_A\n",
      "PLEASANT_V_B\n",
      "R_SANDHSYD_1\n",
      "SANANDRS_9\n",
      "SANA_L_A\n",
      "SANA_V_A\n",
      "SANDHILL_8\n",
      "SANDHSYD\n",
      "SANDHSYD_SH1\n",
      "SANDHSYD_SH3\n",
      "SANDIASUB9W\n",
      "SANDOW1_8N\n",
      "SANDOW1_8W\n",
      "SANMIGL_0025\n",
      "SANMIGL_8609\n",
      "SANO_L_A\n",
      "SANO_V_B\n",
      "SANSOM2_8V\n",
      "SANSOM2_8Z\n",
      "SANTARIT_LD1\n",
      "SANTIAGO_002\n",
      "SANTIAGO_223\n",
      "SOUTHSAN_C\n",
      "SOUTHSAN_KC\n",
      "TNSANDRSON0W\n",
      "TNSANDRSON0X\n",
      "TNSANDRSON0Y\n",
      "WACOSANG1_8X\n",
      "WACOSANG1_8Y\n",
      "LV3_LN\n",
      "LV3_U1\n",
      "EB_HOVEY\n",
      "DUP_DUPV1_G1\n",
      "NTX_NTX_1\n",
      "NTX_NTX_2\n",
      "SCLPCOG_6728\n"
     ]
    }
   ],
   "source": [
    "e = exemplars[0]\n",
    "dfe = df[e]\n",
    "feature_MIs = []\n",
    "idx_wout_1st_week = None\n",
    "print('feature selection for %s' % e)\n",
    "past_index = [24,25,47,48]\n",
    "for c in df.columns:\n",
    "    features = []\n",
    "    targets = []\n",
    "    feature_labels = []\n",
    "    for dt, s in dfe.iteritems():\n",
    "        temp_features = []\n",
    "        pred_hour_index = dfe.index.get_loc(dt)\n",
    "        if type(pred_hour_index) == slice: continue\n",
    "        if pred_hour_index - 48 >= 0:\n",
    "            categorical_features = np.array([work_day_or_holiday(dt), dt.hour, dt.weekday(), dt.month])\n",
    "            targets.append(dfe[pred_hour_index])\n",
    "            for i in past_index:\n",
    "                feature_labels += [c + '_' + str(i)]\n",
    "                temp_features += [df[c].iloc[pred_hour_index-i]]\n",
    "            features.append(temp_features)\n",
    "    features = np.array(features)\n",
    "    targets = np.array(targets)\n",
    "    time, lag = features.shape\n",
    "    for i in range(lag):\n",
    "        f_series = features[:,i]\n",
    "        f_max = np.max(f_series)\n",
    "        f_min = np.min(f_series)\n",
    "        features[:,i] = (f_series-f_min)/(f_max-f_min)\n",
    "    t_max = np.max(targets)\n",
    "    t_min = np.min(targets)\n",
    "    targets = (targets-f_min)/(f_max-f_min)\n",
    "    t_med = np.median(targets)\n",
    "    for k,p in enumerate(targets):\n",
    "        if p >= t_med:\n",
    "            targets[k] = 1.0\n",
    "        else: targets[k] = 0.0\n",
    "    for i in range(lag):\n",
    "        f_med = np.median(features[:,i])\n",
    "        for k,p in enumerate(features[:,i]):\n",
    "            if p >= f_med:\n",
    "                features[:,i][k] = 1.0\n",
    "            else: features[:,i][k] = 0.0\n",
    "    Pt_1 = np.sum(targets)/targets.shape[0]\n",
    "    Pt_0 = 1 - Pt_1\n",
    "    for i in range(lag):\n",
    "        Pf_1 = np.sum(features[:,i])/features[:,i].shape[0]\n",
    "        Pf_0 = 1 - Pf_1\n",
    "        f0_t0 = 0.0\n",
    "        f1_t0 = 0.0\n",
    "        f0_t1 = 0.0\n",
    "        f1_t1 = 0.0\n",
    "        for k,p in enumerate(features[:,i]):\n",
    "            zero=np.float64(0.0)\n",
    "            one=np.float64(1.0)\n",
    "            if (targets[k]==zero) and (p==zero):\n",
    "                f0_t0 += 1.0\n",
    "            if (targets[k]==zero) and (p==one):\n",
    "                f1_t0 += 1.0\n",
    "            if (targets[k]==one) and (p==zero):\n",
    "                f0_t1 += 1.0\n",
    "            if (targets[k]==one) and (p==one):\n",
    "                f1_t1 += 1.0\n",
    "        P00 = f0_t0/features[:,i].shape[0]\n",
    "        P10 = f1_t0/features[:,i].shape[0]\n",
    "        P01 = f0_t1/features[:,i].shape[0]\n",
    "        P11 = f1_t1/features[:,i].shape[0]\n",
    "        if P00==0 or P10==0 or P01==0 or P11 ==0:\n",
    "            MI_i = 0;\n",
    "        else: MI_i = P00*math.log(P00/(Pf_0*Pt_0),2)+\\\n",
    "               P10*math.log(P10/(Pf_1*Pt_0),2)+\\\n",
    "                P01*math.log(P01/(Pf_0*Pt_1),2)+ P11*math.log(P11/(Pf_1*Pt_1),2)\n",
    "        feature_MIs.append((feature_labels[i], MI_i))\n",
    "    print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('feature_MIs.pkl','w+')\n",
    "pickle.dump(feature_MIs, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NCARBIDE_E_2'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_MIs.sort(key=lambda tup: tup[1], reverse=True) \n",
    "selected = filter(lambda x: x[1]>0.45, feature_MIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = qcrr.query(exemplars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "targets = []\n",
    "features = []\n",
    "for pred_hour_index, dt in enumerate(df.index):\n",
    "    if pred_hour_index - 24 >= 0:\n",
    "        categorical_features = [work_day_or_holiday(dt), dt.hour, dt.weekday(), dt.month]\n",
    "        node_names = []\n",
    "        targets.append(df[exemplars[0]].iloc[pred_hour_index])\n",
    "        for s in selected:\n",
    "            node_names.append(s[0][:-3])\n",
    "        numerical_features = df[node_names].iloc[pred_hour_index-24].as_matrix()\n",
    "        f = np.concatenate([numerical_features, categorical_features])\n",
    "        features.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f=open('features.pkl', 'w+')\n",
    "pickle.dump(features, f)\n",
    "f.close()\n",
    "f=open('targets.pkl', 'w+')\n",
    "pickle.dump(targets, f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
