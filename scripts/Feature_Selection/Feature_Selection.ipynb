{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenleejr92/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:27: DeprecationWarning: the sets module is deprecated\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import MySQLdb\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import holidays\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.insert(0, '/Users/kenleejr92/energy_market_project/scripts/MySQL_scripts')\n",
    "from Query_ERCOT_DB import Query_ERCOT_DB\n",
    "import cPickle as pickle\n",
    "from datetime import datetime\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.linalg import Matrix, Matrices\n",
    "import pyspark.mllib.linalg.distributed as pydist\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "import mpld3\n",
    "import re\n",
    "from sets import Set\n",
    "mpld3.enable_notebook()\n",
    "\n",
    "class LMP_Query(Query_ERCOT_DB):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.SPPs = ['HB_BUSAVG', \n",
    "                     'HB_HOUSTON', \n",
    "                     'HB_HUBAVG', \n",
    "                     'HB_NORTH', \n",
    "                     'HB_SOUTH', \n",
    "                     'HB_WEST', \n",
    "                     'LZ_AEN', \n",
    "                     'LZ_CPS', \n",
    "                     'LZ_HOUSTON', \n",
    "                     'LZ_LCRA', \n",
    "                     'LZ_NORTH', \n",
    "                     'LZ_RAYBN', \n",
    "                     'LZ_SOUTH',  \n",
    "                     'LZ_WEST']   \n",
    "        self.node_dict = {}\n",
    "        self.table_list = []\n",
    "        self.df = None\n",
    "        self.CRR_nodes = None\n",
    "        self.table_boundaries = {'table0':('0001', 'BLUEMD1_8X'),\n",
    "                                 'table1':('BLUEMD1_8Z', 'CHT_M'),\n",
    "                                 'table2':('CHT_X', 'DUKE_8405'),\n",
    "                                 'table3':('DUKE_8505', 'ELEVEE_E8'),\n",
    "                                 'table4':('ELEVEE_W8', 'GREENLK_L_A'),\n",
    "                                 'table5':('GREENLK_L_B', 'KEETER'),\n",
    "                                 'table6':('KEITH', 'L_CEDAHI8_1Y'),\n",
    "                                 'table7':('L_CEDAHI8_1Z', 'MOSES_1G'),\n",
    "                                 'table8':('MOSES_2G', 'PHR_8135'),\n",
    "                                 'table9':('PHR_8140', 'SANDOW1_8Y'),\n",
    "                                 'table10':('SANDOW_4G', 'TCN7225_BUS'),\n",
    "                                 'table11': ('TCN7230_BUS', 'VENSW_1777'),\n",
    "                                 'table12':('VENSW_1785', '_WC_V_C')\n",
    "                                 }\n",
    "        for i in range(0,13):\n",
    "            Query_ERCOT_DB.c.execute(\"\"\"SHOW columns FROM DAM_LMP%s\"\"\" % i)\n",
    "            result = [r[0] for r in Query_ERCOT_DB.c.fetchall()[2:]]\n",
    "            self.table_list.append(result)\n",
    "            for node in result:\n",
    "                self.node_dict[node] = i\n",
    "        \n",
    "    \n",
    "    def get_CRR_nodes(self):\n",
    "        Query_ERCOT_DB.c.execute(\"\"\"SELECT DISTINCT Sink FROM crr_ownership ORDER BY Sink\"\"\")\n",
    "        nodes = list(Query_ERCOT_DB.c.fetchall())\n",
    "        CRR_prefixes = [r[0] for r in nodes]\n",
    "        pattern = re.compile('.*_')\n",
    "        matching_patterns = Set()\n",
    "        for idx, node in enumerate(CRR_prefixes):\n",
    "            matches = re.findall(pattern, node)\n",
    "            if matches: matching_patterns.add(matches[0][:-1])\n",
    "        # flatten list of lists      \n",
    "        all_nodes = [item for sublist in self.table_list for item in sublist] + self.SPPs\n",
    "        self.CRR_nodes = []\n",
    "        for pattern in matching_patterns:\n",
    "            pattern2 = re.compile('(.*%s.*)' % pattern)\n",
    "            for node in all_nodes:\n",
    "                if re.search(pattern2, node):\n",
    "                    self.CRR_nodes.append(node)\n",
    "        return self.CRR_nodes\n",
    "    \n",
    "    def query_single_node(self, node):\n",
    "        s=\"\"\"SELECT delivery_date, hour_ending, %s from DAM_LMP%s order by delivery_date, hour_ending\"\"\" % (node, self.node_dict[node])\n",
    "        Query_ERCOT_DB.c.execute(s)\n",
    "        result = list(Query_ERCOT_DB.c.fetchall())\n",
    "        fresult = []\n",
    "        for r in result:\n",
    "            temp = ()\n",
    "            date = r[0]\n",
    "            time = str(int(r[1].split(\":\")[0])-1)\n",
    "            dt = datetime.strptime(date + \" \" + time, \"%Y-%m-%d %H\")\n",
    "            for x in r[2:]:\n",
    "                if x == None: x = 0\n",
    "                temp = temp + (float(x),)\n",
    "            r = (dt,) + temp\n",
    "            fresult.append(r)\n",
    "        self.df = pd.DataFrame(data=[f[1:] for f in fresult], index=[r[0] for r in fresult], columns=[node])\n",
    "        return self.df\n",
    "        \n",
    "    def query(self, nodes, start_date='2011-01-01', end_date='2015-5-23'):\n",
    "        node_string=''\n",
    "        for node in nodes:\n",
    "            node_string = node_string + node + ',' + ' '\n",
    "        node_string = node_string[:-2]\n",
    "        s = \"\"\"SELECT DAM_LMP0.delivery_date, DAM_LMP0.hour_ending, %s \n",
    "                                    from DAM_LMP0 join DAM_LMP1 on (DAM_LMP0.delivery_date = DAM_LMP1.delivery_date and DAM_LMP0.hour_ending = DAM_LMP1.hour_ending) \n",
    "                                    join DAM_LMP2 on (DAM_LMP0.delivery_date = DAM_LMP2.delivery_date and DAM_LMP0.hour_ending = DAM_LMP2.hour_ending)\n",
    "                                    join DAM_LMP3 on (DAM_LMP0.delivery_date = DAM_LMP3.delivery_date and DAM_LMP0.hour_ending = DAM_LMP3.hour_ending)\n",
    "                                    join DAM_LMP4 on (DAM_LMP0.delivery_date = DAM_LMP4.delivery_date and DAM_LMP0.hour_ending = DAM_LMP4.hour_ending)\n",
    "                                    join DAM_LMP5 on (DAM_LMP0.delivery_date = DAM_LMP5.delivery_date and DAM_LMP0.hour_ending = DAM_LMP5.hour_ending)\n",
    "                                    join DAM_LMP6 on (DAM_LMP0.delivery_date = DAM_LMP6.delivery_date and DAM_LMP0.hour_ending = DAM_LMP6.hour_ending)\n",
    "                                    join DAM_LMP7 on (DAM_LMP0.delivery_date = DAM_LMP7.delivery_date and DAM_LMP0.hour_ending = DAM_LMP7.hour_ending)\n",
    "                                    join DAM_LMP8 on (DAM_LMP0.delivery_date = DAM_LMP8.delivery_date and DAM_LMP0.hour_ending = DAM_LMP8.hour_ending)\n",
    "                                    join DAM_LMP9 on (DAM_LMP0.delivery_date = DAM_LMP9.delivery_date and DAM_LMP0.hour_ending = DAM_LMP9.hour_ending)\n",
    "                                    join DAM_LMP10 on (DAM_LMP0.delivery_date = DAM_LMP10.delivery_date and DAM_LMP0.hour_ending = DAM_LMP10.hour_ending)\n",
    "                                    join DAM_LMP11 on (DAM_LMP0.delivery_date = DAM_LMP11.delivery_date and DAM_LMP0.hour_ending = DAM_LMP11.hour_ending)\n",
    "                                    join DAM_LMP12 on (DAM_LMP0.delivery_date = DAM_LMP12.delivery_date and DAM_LMP0.hour_ending = DAM_LMP12.hour_ending)\n",
    "                                    join DAM_SPPs on (DAM_LMP0.delivery_date = DAM_SPPs.delivery_date and DAM_LMP0.hour_ending = DAM_SPPs.hour_ending)\n",
    "                                    where DAM_LMP0.delivery_date > \"%s\" and DAM_LMP0.delivery_date < \"%s\" order by DAM_LMP0.delivery_date, DAM_LMP0.hour_ending;\"\"\" % (node_string, start_date, end_date)\n",
    "        Query_ERCOT_DB.c.execute(s)\n",
    "        result = list(Query_ERCOT_DB.c.fetchall())\n",
    "        fresult = []\n",
    "        for r in result:\n",
    "            temp = ()\n",
    "            date = r[0]\n",
    "            time = str(int(r[1].split(\":\")[0])-1)\n",
    "            dt = datetime.strptime(date + \" \" + time, \"%Y-%m-%d %H\")\n",
    "            for x in r[2:]:\n",
    "                if x == None: x = 0\n",
    "                temp = temp + (float(x),)\n",
    "            r = (dt,) + temp\n",
    "            fresult.append(r)\n",
    "        self.df = pd.DataFrame(data=[f[1:] for f in fresult], index=[r[0] for r in fresult], columns=nodes)\n",
    "        return self.df\n",
    "        \n",
    "    def get_price(self, node, date, hour_ending):\n",
    "        for i in range(0,13):\n",
    "            node = append_n(node)\n",
    "            if node in self.table_columns['table%s' % i]:\n",
    "                Query_ERCOT_DB.c.execute(\"\"\"SELECT %s FROM DAM_LMP%s WHERE delivery_date = \"%s\" AND hour_ending = \\\"%s\\\"\"\"\" % (node, i, date, hour_ending))\n",
    "                result = list(Query_ERCOT_DB.c.fetchall())[0][0]\n",
    "                return result\n",
    "        \n",
    "\n",
    "def append_n(name):\n",
    "    if name[0] in ['0','1','2','3','4','5','6','7','8','9'] or name == 'LOAD':\n",
    "        name = 'n' + name\n",
    "    return name\n",
    "\n",
    "def dist(x,y):\n",
    "    cost = 0\n",
    "    for i in range(len(x)):\n",
    "        if x[i] == 0 or y[i] == 0:\n",
    "            continue\n",
    "        else:\n",
    "            cost = cost + np.abs(y[i] - x[i])[0]\n",
    "    return cost\n",
    "\n",
    "def string_to_date(string_date):\n",
    "    return datetime.strptime(string_date, \"%Y-%m-%d %H\")\n",
    "\n",
    "def date_to_string(date):\n",
    "    return date.strftime(\"%Y-%m-%d %H\")\n",
    "\n",
    "def weekday_of_date(date):\n",
    "    return calendar.day_name[date.weekday()]\n",
    "\n",
    "def work_day_or_holiday(date):\n",
    "    us_holidays = holidays.UnitedStates()\n",
    "    if date in us_holidays or weekday_of_date(date) == \"Sunday\" or weekday_of_date(date) == \"Saturday\":\n",
    "        return int(1)\n",
    "    else: return int(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"spark_DAM_correlation\")\n",
    "sc = SparkContext(conf = conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving saved\n"
     ]
    }
   ],
   "source": [
    "qcrr = LMP_Query()\n",
    "if not os.path.isfile('crr_nodes.pkl'):\n",
    "    print('making new')\n",
    "    crr_nodes = qcrr.get_CRR_nodes()\n",
    "    f = open('crr_nodes.pkl', 'w+')\n",
    "    pickle.dump(crr_nodes, f)\n",
    "    f.close()\n",
    "else:\n",
    "    print('retrieving saved')\n",
    "    f = open('crr_nodes.pkl', 'r')\n",
    "    crr_nodes = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile('crr_node_prices.pkl'):\n",
    "    print('making new')\n",
    "    x1 = qcrr.query(crr_nodes[0:500]).as_matrix()\n",
    "    x2 = qcrr.query(crr_nodes[500:1000]).as_matrix()\n",
    "    x3 = qcrr.query(crr_nodes[1000:1500]).as_matrix()\n",
    "    x4 = qcrr.query(crr_nodes[1500:2000]).as_matrix()\n",
    "    x5 = qcrr.query(crr_nodes[2000:2500]).as_matrix()\n",
    "    x6 = qcrr.query(crr_nodes[2500:3000]).as_matrix()\n",
    "    x7 = qcrr.query(crr_nodes[3000:]).as_matrix()\n",
    "    x8 = np.concatenate((x1,x2), axis=1)\n",
    "    x8 = np.concatenate((x8,x3), axis=1)\n",
    "    x8 = np.concatenate((x8,x4), axis=1)\n",
    "    x8 = np.concatenate((x8,x5), axis=1)\n",
    "    x8 = np.concatenate((x8,x6), axis=1)\n",
    "    x8 = np.concatenate((x8,x7), axis=1)\n",
    "    f = open('crr_node_prices.pkl', 'w+')\n",
    "    pickle.dump(x8, f)\n",
    "    crr_prices = x8\n",
    "    f.close()\n",
    "else:\n",
    "    print('retrieving saved')\n",
    "    f = open('crr_node_prices.pkl', 'r')\n",
    "    crr_prices = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1b1901eb41bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sim_mat.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sim_mat.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msim_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0maf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAffinityPropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maffinity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'precomputed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('sim_mat.pkl'):\n",
    "    sample_size = crr_prices.shape[0]\n",
    "    train_size = 0.8\n",
    "    train_split = int(0.8*sample_size)\n",
    "    train_set = crr_prices[0:train_split,:]\n",
    "    sim_mat = np.zeros((len(crr_nodes),len(crr_nodes)))\n",
    "    for i in range(len(crr_nodes)):\n",
    "        for j in np.arange(i,len(crr_nodes)):\n",
    "            sim_mat[i,j] = np.negative(np.sum(np.square(train_set[:,i] - train_set[:,j])))\n",
    "\n",
    "    for i in range(len(crr_nodes)):\n",
    "        for j in np.arange(i,len(crr_nodes)):\n",
    "            sim_mat[j,i] = sim_mat[i,j]\n",
    "    with open('sim_mat.pkl', 'w+') as f: pickle.dump(sim_mat, f)\n",
    "else: \n",
    "    with open('sim_mat.pkl', 'r') as f: sim_mat = pickle.load(f)\n",
    "\n",
    "af = AffinityPropagation(affinity='precomputed').fit(sim_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_df = pd.DataFrame.from_csv('/Users/kenleejr92/energy_market_project/crr_node_correlation_matrix.csv', header=None, index_col=None)\n",
    "x = x_df.as_matrix()\n",
    "for i, item in enumerate(x[:,0]):\n",
    "    x[i,0] = float(item[1:])\n",
    "for i, item in enumerate(x[:,3587]):\n",
    "    x[i,3587] = float(item[:-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_dict = {}\n",
    "for i, col in enumerate(crr_nodes):\n",
    "    col_dict[i] = col\n",
    "if not os.path.isfile('cluster_list.pkl'):\n",
    "    cluster_list = [[] for i in range(af.cluster_centers_indices_.size)]\n",
    "    for idx, item in enumerate(af.labels_):\n",
    "        cluster_list[item].append(col_dict[idx])\n",
    "    f = open('cluster_list.pkl', 'w+')\n",
    "    pickle.dump(cluster_list, f)\n",
    "    f.close()\n",
    "else:\n",
    "    f = open('cluster_list.pkl', 'r')\n",
    "    cluster_list = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "if not os.path.isfile('exemplars.pkl'):\n",
    "    exemplars = []\n",
    "    for idx, cluster in enumerate(cluster_list):\n",
    "        exemplars.append(col_dict[idx])\n",
    "    f = open('exemplars.pkl', 'w+')\n",
    "    pickle.dump(exemplars, f)\n",
    "    f.close()\n",
    "else:\n",
    "    f = open('exemplars.pkl', 'r')\n",
    "    exemplars = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1 = qcrr.query(exemplars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "non_zero_exemplars = []\n",
    "mat = df1.as_matrix()\n",
    "mat.shape\n",
    "for i in range(mat.shape[1]):\n",
    "    series = mat[:,i]\n",
    "    zero_count=0\n",
    "    for j in series:\n",
    "        if j==0.0:\n",
    "            zero_count+=1\n",
    "    if zero_count<500:\n",
    "        non_zero_exemplars.append(exemplars[i])\n",
    "df = df1[non_zero_exemplars]\n",
    "ax = df.plot()\n",
    "ax.set_xlabel('Hour')\n",
    "ax.set_ylabel('Dollar/MWh')\n",
    "ax.legend_.remove()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature selection for MILDRED2_9X\n",
      "HAMILTO_0001\n",
      "MIL1_EB\n",
      "MILDRED2_9X\n",
      "MILHWY_EB\n",
      "MILLCR_L_A\n",
      "EXN_EPS_1\n",
      "OGSES_AG2\n",
      "GUADG_GAS3\n",
      "R_GUADG_4\n",
      "STEAM_EB_3\n",
      "STEA_STEAM_1\n",
      "DIB_DIB_G2\n",
      "FORSAN_8\n",
      "LV3_U1\n",
      "DUP_DUPV1_G1\n",
      "NTX_NTX_1\n"
     ]
    }
   ],
   "source": [
    "exemplars = non_zero_exemplars\n",
    "mat = df[exemplars].as_matrix()\n",
    "\n",
    "dfe = df[e]\n",
    "feature_MIs = []\n",
    "print('feature selection for %s' % e)\n",
    "past_index = [24,25,47,48]\n",
    "for c in df.columns:\n",
    "    features = []\n",
    "    targets = []\n",
    "    feature_labels = []\n",
    "    for dt, s in dfe.iteritems():\n",
    "        temp_features = []\n",
    "        pred_hour_index = dfe.index.get_loc(dt)\n",
    "        if type(pred_hour_index) == slice: continue\n",
    "        if pred_hour_index - 48 >= 0:\n",
    "            targets.append(dfe[pred_hour_index])\n",
    "            for i in past_index:\n",
    "                feature_labels += [c + '_' + str(i)]\n",
    "                temp_features += [df[c].iloc[pred_hour_index-i]]\n",
    "            features.append(temp_features)\n",
    "    features = np.array(features)\n",
    "    targets = np.array(targets)\n",
    "    time, lag = features.shape\n",
    "    for i in range(lag):\n",
    "        f_series = features[:,i]\n",
    "        f_max = np.max(f_series)\n",
    "        f_min = np.min(f_series)\n",
    "        features[:,i] = (f_series-f_min)/(f_max-f_min)\n",
    "    t_max = np.max(targets)\n",
    "    t_min = np.min(targets)\n",
    "    targets = (targets-f_min)/(f_max-f_min)\n",
    "    t_med = np.median(targets)\n",
    "    for k,p in enumerate(targets):\n",
    "        if p >= t_med:\n",
    "            targets[k] = 1.0\n",
    "        else: targets[k] = 0.0\n",
    "    for i in range(lag):\n",
    "        f_med = np.median(features[:,i])\n",
    "        for k,p in enumerate(features[:,i]):\n",
    "            if p >= f_med:\n",
    "                features[:,i][k] = 1.0\n",
    "            else: features[:,i][k] = 0.0\n",
    "    Pt_1 = np.sum(targets)/targets.shape[0]\n",
    "    Pt_0 = 1 - Pt_1\n",
    "    for i in range(lag):\n",
    "        Pf_1 = np.sum(features[:,i])/features[:,i].shape[0]\n",
    "        Pf_0 = 1 - Pf_1\n",
    "        f0_t0 = 0.0\n",
    "        f1_t0 = 0.0\n",
    "        f0_t1 = 0.0\n",
    "        f1_t1 = 0.0\n",
    "        for k,p in enumerate(features[:,i]):\n",
    "            zero=np.float64(0.0)\n",
    "            one=np.float64(1.0)\n",
    "            if (targets[k]==zero) and (p==zero):\n",
    "                f0_t0 += 1.0\n",
    "            if (targets[k]==zero) and (p==one):\n",
    "                f1_t0 += 1.0\n",
    "            if (targets[k]==one) and (p==zero):\n",
    "                f0_t1 += 1.0\n",
    "            if (targets[k]==one) and (p==one):\n",
    "                f1_t1 += 1.0\n",
    "        P00 = f0_t0/features[:,i].shape[0]\n",
    "        P10 = f1_t0/features[:,i].shape[0]\n",
    "        P01 = f0_t1/features[:,i].shape[0]\n",
    "        P11 = f1_t1/features[:,i].shape[0]\n",
    "        if P00==0 or P10==0 or P01==0 or P11 ==0:\n",
    "            MI_i = 0\n",
    "        else: MI_i = P00*math.log(P00/(Pf_0*Pt_0),2)+\\\n",
    "               P10*math.log(P10/(Pf_1*Pt_0),2)+\\\n",
    "                P01*math.log(P01/(Pf_0*Pt_1),2)+ P11*math.log(P11/(Pf_1*Pt_1),2)\n",
    "        feature_MIs.append((feature_labels[i], MI_i))\n",
    "    print(c)\n",
    "f = open('%s_MIs.pkl' % e,'w+')\n",
    "pickle.dump(feature_MIs, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('feature_MIs.pkl','w+')\n",
    "pickle.dump(feature_MIs, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('MILDRED2_9X_24', 0.485370103164328),\n",
       " ('STEA_STEAM_1_24', 0.4791888507268204),\n",
       " ('OGSES_AG2_24', 0.47759641048144374),\n",
       " ('NTX_NTX_1_24', 0.47714213490863633),\n",
       " ('GUADG_GAS3_24', 0.4768398910293509),\n",
       " ('R_GUADG_4_24', 0.4768398910293509),\n",
       " ('EXN_EPS_1_24', 0.4728443278381428),\n",
       " ('DIB_DIB_G2_24', 0.47284405909285193),\n",
       " ('DUP_DUPV1_G1_24', 0.46167723626716456),\n",
       " ('MILLCR_L_A_24', 0.45785369429614),\n",
       " ('HAMILTO_0001_24', 0.41082360828617914),\n",
       " ('MILDRED2_9X_48', 0.39502018464001765),\n",
       " ('OGSES_AG2_48', 0.39403506915132),\n",
       " ('GUADG_GAS3_48', 0.39324811449477676),\n",
       " ('R_GUADG_4_48', 0.39324811449477676),\n",
       " ('STEA_STEAM_1_48', 0.3923311848700755),\n",
       " ('NTX_NTX_1_48', 0.39043666011167066),\n",
       " ('EXN_EPS_1_48', 0.3903713858319523),\n",
       " ('DIB_DIB_G2_48', 0.39024096193792857),\n",
       " ('DIB_DIB_G2_25', 0.3790775518182275),\n",
       " ('DUP_DUPV1_G1_48', 0.37824776029741),\n",
       " ('EXN_EPS_1_25', 0.3771648448754096),\n",
       " ('GUADG_GAS3_25', 0.3758298601136147),\n",
       " ('R_GUADG_4_25', 0.3758298601136147),\n",
       " ('MILDRED2_9X_25', 0.3758295743849692),\n",
       " ('MILLCR_L_A_48', 0.3751951349158334),\n",
       " ('NTX_NTX_1_25', 0.3720332754597774),\n",
       " ('STEA_STEAM_1_25', 0.3699567296013673),\n",
       " ('FORSAN_8_24', 0.3698312658736244),\n",
       " ('OGSES_AG2_25', 0.3687646984906764),\n",
       " ('DUP_DUPV1_G1_25', 0.3682010737986475),\n",
       " ('MILLCR_L_A_25', 0.3613588184293719),\n",
       " ('HAMILTO_0001_25', 0.347634313252295),\n",
       " ('HAMILTO_0001_48', 0.3473936136057514),\n",
       " ('OGSES_AG2_47', 0.32192697982494967),\n",
       " ('MILDRED2_9X_47', 0.3210693465054587),\n",
       " ('STEA_STEAM_1_47', 0.31918855055811357),\n",
       " ('NTX_NTX_1_47', 0.3174279401732184),\n",
       " ('GUADG_GAS3_47', 0.3162392103460364),\n",
       " ('R_GUADG_4_47', 0.3162392103460364),\n",
       " ('EXN_EPS_1_47', 0.30838635462109754),\n",
       " ('FORSAN_8_25', 0.30733065717779184),\n",
       " ('DIB_DIB_G2_47', 0.3070529618472586),\n",
       " ('FORSAN_8_48', 0.3068311862532641),\n",
       " ('DUP_DUPV1_G1_47', 0.30351558059050776),\n",
       " ('MILLCR_L_A_47', 0.3008799055074767),\n",
       " ('HAMILTO_0001_47', 0.2830480702850067),\n",
       " ('STEAM_EB_3_24', 0.27464714532191514),\n",
       " ('FORSAN_8_47', 0.24930762518528504),\n",
       " ('STEAM_EB_3_48', 0.23392754082574116),\n",
       " ('STEAM_EB_3_25', 0.2166627541351358),\n",
       " ('STEAM_EB_3_47', 0.20128897979537055),\n",
       " ('MIL1_EB_24', 0),\n",
       " ('MIL1_EB_25', 0),\n",
       " ('MIL1_EB_47', 0),\n",
       " ('MIL1_EB_48', 0),\n",
       " ('MILHWY_EB_24', 0),\n",
       " ('MILHWY_EB_25', 0),\n",
       " ('MILHWY_EB_47', 0),\n",
       " ('MILHWY_EB_48', 0),\n",
       " ('LV3_U1_24', 0),\n",
       " ('LV3_U1_25', 0),\n",
       " ('LV3_U1_47', 0),\n",
       " ('LV3_U1_48', 0)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('feature_MIs.pkl', 'r')\n",
    "feature_MIs = pickle.load(f)\n",
    "f.close()\n",
    "feature_MIs.sort(key=lambda tup: tup[1], reverse=True) \n",
    "selected = filter(lambda x: x[1]>0.3, feature_MIs)\n",
    "feature_MIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = qcrr.query(exemplars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "targets = []\n",
    "features = []\n",
    "for pred_hour_index, dt in enumerate(df.index):\n",
    "    if pred_hour_index - 48 >= 0:\n",
    "        categorical_features = [work_day_or_holiday(dt), dt.hour, dt.weekday(), dt.month]\n",
    "        numerical_features = []\n",
    "        nodes = []\n",
    "        targets.append(df[exemplars[0]].iloc[pred_hour_index])\n",
    "        for s in selected:\n",
    "            nodes.append((s[0][:-3], int(s[0][-2:])))\n",
    "        for n in nodes:\n",
    "            numerical_features.append(df[n[0]].iloc[pred_hour_index-n[1]])\n",
    "        f = np.concatenate([numerical_features, categorical_features])\n",
    "        features.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f=open('features.pkl', 'w+')\n",
    "pickle.dump(features, f)\n",
    "f.close()\n",
    "f=open('targets.pkl', 'w+')\n",
    "pickle.dump(targets, f)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
