{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenleejr92/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:28: DeprecationWarning: the sets module is deprecated\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import MySQLdb\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import holidays\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.insert(0, '/Users/kenleejr92/energy_market_project/scripts/MySQL_scripts')\n",
    "from Query_ERCOT_DB import Query_ERCOT_DB\n",
    "import cPickle as pickle\n",
    "from datetime import datetime\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.linalg import Matrix, Matrices\n",
    "import pyspark.mllib.linalg.distributed as pydist\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn import preprocessing\n",
    "import mpld3\n",
    "import re\n",
    "from sets import Set\n",
    "mpld3.enable_notebook()\n",
    "\n",
    "class LMP_Query(Query_ERCOT_DB):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.SPPs = ['HB_BUSAVG', \n",
    "                     'HB_HOUSTON', \n",
    "                     'HB_HUBAVG', \n",
    "                     'HB_NORTH', \n",
    "                     'HB_SOUTH', \n",
    "                     'HB_WEST', \n",
    "                     'LZ_AEN', \n",
    "                     'LZ_CPS', \n",
    "                     'LZ_HOUSTON', \n",
    "                     'LZ_LCRA', \n",
    "                     'LZ_NORTH', \n",
    "                     'LZ_RAYBN', \n",
    "                     'LZ_SOUTH',  \n",
    "                     'LZ_WEST']   \n",
    "        self.node_dict = {}\n",
    "        self.table_list = []\n",
    "        self.df = None\n",
    "        self.CRR_nodes = None\n",
    "        self.table_boundaries = {'table0':('0001', 'BLUEMD1_8X'),\n",
    "                                 'table1':('BLUEMD1_8Z', 'CHT_M'),\n",
    "                                 'table2':('CHT_X', 'DUKE_8405'),\n",
    "                                 'table3':('DUKE_8505', 'ELEVEE_E8'),\n",
    "                                 'table4':('ELEVEE_W8', 'GREENLK_L_A'),\n",
    "                                 'table5':('GREENLK_L_B', 'KEETER'),\n",
    "                                 'table6':('KEITH', 'L_CEDAHI8_1Y'),\n",
    "                                 'table7':('L_CEDAHI8_1Z', 'MOSES_1G'),\n",
    "                                 'table8':('MOSES_2G', 'PHR_8135'),\n",
    "                                 'table9':('PHR_8140', 'SANDOW1_8Y'),\n",
    "                                 'table10':('SANDOW_4G', 'TCN7225_BUS'),\n",
    "                                 'table11': ('TCN7230_BUS', 'VENSW_1777'),\n",
    "                                 'table12':('VENSW_1785', '_WC_V_C')\n",
    "                                 }\n",
    "        for i in range(0,13):\n",
    "            Query_ERCOT_DB.c.execute(\"\"\"SHOW columns FROM DAM_LMP%s\"\"\" % i)\n",
    "            result = [r[0] for r in Query_ERCOT_DB.c.fetchall()[2:]]\n",
    "            self.table_list.append(result)\n",
    "            for node in result:\n",
    "                self.node_dict[node] = i\n",
    "        \n",
    "    \n",
    "    def get_CRR_nodes(self):\n",
    "        Query_ERCOT_DB.c.execute(\"\"\"SELECT DISTINCT Sink FROM crr_ownership ORDER BY Sink\"\"\")\n",
    "        nodes = list(Query_ERCOT_DB.c.fetchall())\n",
    "        CRR_prefixes = [r[0] for r in nodes]\n",
    "        pattern = re.compile('.*_')\n",
    "        matching_patterns = Set()\n",
    "        for idx, node in enumerate(CRR_prefixes):\n",
    "            matches = re.findall(pattern, node)\n",
    "            if matches: matching_patterns.add(matches[0][:-1])\n",
    "        # flatten list of lists      \n",
    "        all_nodes = [item for sublist in self.table_list for item in sublist] + self.SPPs\n",
    "        self.CRR_nodes = []\n",
    "        for pattern in matching_patterns:\n",
    "            pattern2 = re.compile('(.*%s.*)' % pattern)\n",
    "            for node in all_nodes:\n",
    "                if re.search(pattern2, node):\n",
    "                    self.CRR_nodes.append(node)\n",
    "        return self.CRR_nodes\n",
    "    \n",
    "    def query_single_node(self, node):\n",
    "        s=\"\"\"SELECT delivery_date, hour_ending, %s from DAM_LMP%s order by delivery_date, hour_ending\"\"\" % (node, self.node_dict[node])\n",
    "        Query_ERCOT_DB.c.execute(s)\n",
    "        result = list(Query_ERCOT_DB.c.fetchall())\n",
    "        fresult = []\n",
    "        for r in result:\n",
    "            temp = ()\n",
    "            date = r[0]\n",
    "            time = str(int(r[1].split(\":\")[0])-1)\n",
    "            dt = datetime.strptime(date + \" \" + time, \"%Y-%m-%d %H\")\n",
    "            for x in r[2:]:\n",
    "                if x == None: x = 0\n",
    "                temp = temp + (float(x),)\n",
    "            r = (dt,) + temp\n",
    "            fresult.append(r)\n",
    "        self.df = pd.DataFrame(data=[f[1:] for f in fresult], index=[r[0] for r in fresult], columns=[node])\n",
    "        return self.df\n",
    "        \n",
    "    def query(self, nodes, start_date='2011-01-01', end_date='2015-5-23'):\n",
    "        node_string=''\n",
    "        for node in nodes:\n",
    "            node_string = node_string + node + ',' + ' '\n",
    "        node_string = node_string[:-2]\n",
    "        s = \"\"\"SELECT DAM_LMP0.delivery_date, DAM_LMP0.hour_ending, %s \n",
    "                                    from DAM_LMP0 join DAM_LMP1 on (DAM_LMP0.delivery_date = DAM_LMP1.delivery_date and DAM_LMP0.hour_ending = DAM_LMP1.hour_ending) \n",
    "                                    join DAM_LMP2 on (DAM_LMP0.delivery_date = DAM_LMP2.delivery_date and DAM_LMP0.hour_ending = DAM_LMP2.hour_ending)\n",
    "                                    join DAM_LMP3 on (DAM_LMP0.delivery_date = DAM_LMP3.delivery_date and DAM_LMP0.hour_ending = DAM_LMP3.hour_ending)\n",
    "                                    join DAM_LMP4 on (DAM_LMP0.delivery_date = DAM_LMP4.delivery_date and DAM_LMP0.hour_ending = DAM_LMP4.hour_ending)\n",
    "                                    join DAM_LMP5 on (DAM_LMP0.delivery_date = DAM_LMP5.delivery_date and DAM_LMP0.hour_ending = DAM_LMP5.hour_ending)\n",
    "                                    join DAM_LMP6 on (DAM_LMP0.delivery_date = DAM_LMP6.delivery_date and DAM_LMP0.hour_ending = DAM_LMP6.hour_ending)\n",
    "                                    join DAM_LMP7 on (DAM_LMP0.delivery_date = DAM_LMP7.delivery_date and DAM_LMP0.hour_ending = DAM_LMP7.hour_ending)\n",
    "                                    join DAM_LMP8 on (DAM_LMP0.delivery_date = DAM_LMP8.delivery_date and DAM_LMP0.hour_ending = DAM_LMP8.hour_ending)\n",
    "                                    join DAM_LMP9 on (DAM_LMP0.delivery_date = DAM_LMP9.delivery_date and DAM_LMP0.hour_ending = DAM_LMP9.hour_ending)\n",
    "                                    join DAM_LMP10 on (DAM_LMP0.delivery_date = DAM_LMP10.delivery_date and DAM_LMP0.hour_ending = DAM_LMP10.hour_ending)\n",
    "                                    join DAM_LMP11 on (DAM_LMP0.delivery_date = DAM_LMP11.delivery_date and DAM_LMP0.hour_ending = DAM_LMP11.hour_ending)\n",
    "                                    join DAM_LMP12 on (DAM_LMP0.delivery_date = DAM_LMP12.delivery_date and DAM_LMP0.hour_ending = DAM_LMP12.hour_ending)\n",
    "                                    join DAM_SPPs on (DAM_LMP0.delivery_date = DAM_SPPs.delivery_date and DAM_LMP0.hour_ending = DAM_SPPs.hour_ending)\n",
    "                                    where DAM_LMP0.delivery_date > \"%s\" and DAM_LMP0.delivery_date < \"%s\" order by DAM_LMP0.delivery_date, DAM_LMP0.hour_ending;\"\"\" % (node_string, start_date, end_date)\n",
    "        Query_ERCOT_DB.c.execute(s)\n",
    "        result = list(Query_ERCOT_DB.c.fetchall())\n",
    "        fresult = []\n",
    "        for r in result:\n",
    "            temp = ()\n",
    "            date = r[0]\n",
    "            time = str(int(r[1].split(\":\")[0])-1)\n",
    "            dt = datetime.strptime(date + \" \" + time, \"%Y-%m-%d %H\")\n",
    "            for x in r[2:]:\n",
    "                if x == None: x = 0\n",
    "                temp = temp + (float(x),)\n",
    "            r = (dt,) + temp\n",
    "            fresult.append(r)\n",
    "        self.df = pd.DataFrame(data=[f[1:] for f in fresult], index=[r[0] for r in fresult], columns=nodes)\n",
    "        return self.df\n",
    "        \n",
    "    def get_price(self, node, date, hour_ending):\n",
    "        for i in range(0,13):\n",
    "            node = append_n(node)\n",
    "            if node in self.table_columns['table%s' % i]:\n",
    "                Query_ERCOT_DB.c.execute(\"\"\"SELECT %s FROM DAM_LMP%s WHERE delivery_date = \"%s\" AND hour_ending = \\\"%s\\\"\"\"\" % (node, i, date, hour_ending))\n",
    "                result = list(Query_ERCOT_DB.c.fetchall())[0][0]\n",
    "                return result\n",
    "        \n",
    "\n",
    "def append_n(name):\n",
    "    if name[0] in ['0','1','2','3','4','5','6','7','8','9'] or name == 'LOAD':\n",
    "        name = 'n' + name\n",
    "    return name\n",
    "\n",
    "def dist(x,y):\n",
    "    cost = 0\n",
    "    for i in range(len(x)):\n",
    "        if x[i] == 0 or y[i] == 0:\n",
    "            continue\n",
    "        else:\n",
    "            cost = cost + np.abs(y[i] - x[i])[0]\n",
    "    return cost\n",
    "\n",
    "def string_to_date(string_date):\n",
    "    return datetime.strptime(string_date, \"%Y-%m-%d %H\")\n",
    "\n",
    "def date_to_string(date):\n",
    "    return date.strftime(\"%Y-%m-%d %H\")\n",
    "\n",
    "def weekday_of_date(date):\n",
    "    return calendar.day_name[date.weekday()]\n",
    "\n",
    "def work_day_or_holiday(date):\n",
    "    us_holidays = holidays.UnitedStates()\n",
    "    if date in us_holidays or weekday_of_date(date) == \"Sunday\" or weekday_of_date(date) == \"Saturday\":\n",
    "        return int(1)\n",
    "    else: return int(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"spark_DAM_correlation\")\n",
    "sc = SparkContext(conf = conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving saved\n"
     ]
    }
   ],
   "source": [
    "qcrr = LMP_Query()\n",
    "if not os.path.isfile('crr_nodes.pkl'):\n",
    "    print('making new')\n",
    "    crr_nodes = qcrr.get_CRR_nodes()\n",
    "    f = open('crr_nodes.pkl', 'w+')\n",
    "    pickle.dump(crr_nodes, f)\n",
    "    f.close()\n",
    "else:\n",
    "    print('retrieving saved')\n",
    "    f = open('crr_nodes.pkl', 'r')\n",
    "    crr_nodes = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.isfile('crr_node_prices.pkl'):\n",
    "    print('making new')\n",
    "    x1 = qcrr.query(crr_nodes[0:500]).as_matrix()\n",
    "    x2 = qcrr.query(crr_nodes[500:1000]).as_matrix()\n",
    "    x3 = qcrr.query(crr_nodes[1000:1500]).as_matrix()\n",
    "    x4 = qcrr.query(crr_nodes[1500:2000]).as_matrix()\n",
    "    x5 = qcrr.query(crr_nodes[2000:2500]).as_matrix()\n",
    "    x6 = qcrr.query(crr_nodes[2500:3000]).as_matrix()\n",
    "    x7 = qcrr.query(crr_nodes[3000:]).as_matrix()\n",
    "    x8 = np.concatenate((x1,x2), axis=1)\n",
    "    x8 = np.concatenate((x8,x3), axis=1)\n",
    "    x8 = np.concatenate((x8,x4), axis=1)\n",
    "    x8 = np.concatenate((x8,x5), axis=1)\n",
    "    x8 = np.concatenate((x8,x6), axis=1)\n",
    "    x8 = np.concatenate((x8,x7), axis=1)\n",
    "    f = open('crr_node_prices.pkl', 'w+')\n",
    "    pickle.dump(x8, f)\n",
    "    crr_prices = x8\n",
    "    f.close()\n",
    "else:\n",
    "    print('retrieving saved')\n",
    "    f = open('crr_node_prices.pkl', 'r')\n",
    "    crr_prices = pickle.load(f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1b1901eb41bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sim_mat.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_mat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sim_mat.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msim_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0maf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAffinityPropagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maffinity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'precomputed'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msim_mat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not os.path.isfile('sim_mat.pkl'):\n",
    "    sample_size = crr_prices.shape[0]\n",
    "    train_size = 0.8\n",
    "    train_split = int(0.8*sample_size)\n",
    "    train_set = crr_prices[0:train_split,:]\n",
    "    sim_mat = np.zeros((len(crr_nodes),len(crr_nodes)))\n",
    "    for i in range(len(crr_nodes)):\n",
    "        for j in np.arange(i,len(crr_nodes)):\n",
    "            sim_mat[i,j] = np.negative(np.sum(np.square(train_set[:,i] - train_set[:,j])))\n",
    "\n",
    "    for i in range(len(crr_nodes)):\n",
    "        for j in np.arange(i,len(crr_nodes)):\n",
    "            sim_mat[j,i] = sim_mat[i,j]\n",
    "    with open('sim_mat.pkl', 'w+') as f: pickle.dump(sim_mat, f)\n",
    "else: \n",
    "    with open('sim_mat.pkl', 'r') as f: sim_mat = pickle.load(f)\n",
    "\n",
    "af = AffinityPropagation(affinity='precomputed').fit(sim_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_df = pd.DataFrame.from_csv('/Users/kenleejr92/energy_market_project/crr_node_correlation_matrix.csv', header=None, index_col=None)\n",
    "x = x_df.as_matrix()\n",
    "for i, item in enumerate(x[:,0]):\n",
    "    x[i,0] = float(item[1:])\n",
    "for i, item in enumerate(x[:,3587]):\n",
    "    x[i,3587] = float(item[:-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_dict = {}\n",
    "for i, col in enumerate(crr_nodes):\n",
    "    col_dict[i] = col\n",
    "if not os.path.isfile('cluster_list.pkl'):\n",
    "    cluster_list = [[] for i in range(af.cluster_centers_indices_.size)]\n",
    "    for idx, item in enumerate(af.labels_):\n",
    "        cluster_list[item].append(col_dict[idx])\n",
    "    f = open('cluster_list.pkl', 'w+')\n",
    "    pickle.dump(cluster_list, f)\n",
    "    f.close()\n",
    "else:\n",
    "    f = open('cluster_list.pkl', 'r')\n",
    "    cluster_list = pickle.load(f)\n",
    "    f.close()\n",
    "    \n",
    "if not os.path.isfile('exemplars.pkl'):\n",
    "    exemplars = []\n",
    "    for idx, cluster in enumerate(cluster_list):\n",
    "        exemplars.append(col_dict[idx])\n",
    "    f = open('exemplars.pkl', 'w+')\n",
    "    pickle.dump(exemplars, f)\n",
    "    f.close()\n",
    "else:\n",
    "    f = open('exemplars.pkl', 'r')\n",
    "    exemplars = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "df1 = qcrr.query(exemplars)\n",
    "mat = df1.as_matrix()\n",
    "non_zero_exemplars = []\n",
    "for i in range(mat.shape[1]):\n",
    "    series = mat[:,i]\n",
    "    zero_count=0\n",
    "    for j in series:\n",
    "        if j==0.0:\n",
    "            zero_count+=1\n",
    "    if zero_count<500:\n",
    "        non_zero_exemplars.append(exemplars[i])\n",
    "df = df1[non_zero_exemplars]\n",
    "# ax = df.plot()\n",
    "# ax.set_xlabel('Hour')\n",
    "# ax.set_ylabel('Dollar/MWh')\n",
    "# ax.legend_.remove()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature selection for CAPRIDGE_CR1\n"
     ]
    }
   ],
   "source": [
    "exemplars = non_zero_exemplars\n",
    "mat = df[exemplars].as_matrix()\n",
    "train_size = 0.8\n",
    "sample_size = mat.shape[0]\n",
    "train_split = int(0.8*sample_size)\n",
    "exemplar_prices = mat[0:train_split,:]\n",
    "scaler = preprocessing.MinMaxScaler((0,1))\n",
    "exemplar_prices = scaler.fit_transform(exemplar_prices)\n",
    "medians = np.median(exemplar_prices, axis=0)\n",
    "def quantize(a):\n",
    "    a_med = np.median(a)\n",
    "    for idx, i in enumerate(a):\n",
    "        if i >= a_med:\n",
    "            a[idx] = 1.0\n",
    "        else: a[idx] = 0.0\n",
    "    return a\n",
    "exemplar_prices = np.apply_along_axis(quantize, 0, exemplar_prices)\n",
    "\n",
    "def MI(a,b):\n",
    "    p00 = 0.0\n",
    "    p10 = 0.0\n",
    "    p01 = 0.0\n",
    "    p11 = 0.0\n",
    "    MI = 0.0\n",
    "    zero=np.float64(0.0)\n",
    "    one=np.float64(1.0)\n",
    "    for i in range(len(a)):\n",
    "        if a[i]+b[i]==0.0:\n",
    "            p00 += 1.0\n",
    "        if a[i]==zero and b[i]==one:\n",
    "            p10 += 1.0\n",
    "        if (a[i]==one) and (b[i]==zero):\n",
    "            p01 += 1.0\n",
    "        if a[i]+b[i]==2.0:\n",
    "            p11 += 1.0\n",
    "    p00 = p00/train_split\n",
    "    p10 = p10/train_split\n",
    "    p01 = p01/train_split\n",
    "    p11 = p11/train_split\n",
    "    if p00==0 or p10==0 or p01==0 or p11 ==0:\n",
    "        return 0.0\n",
    "    else: return p00*math.log(p00/0.25,2)+\\\n",
    "           p10*math.log(p10/0.25,2)+\\\n",
    "            p01*math.log(p01/0.25,2)+ p11*math.log(p11/0.25,2)\n",
    "\n",
    "e = df.columns[0]\n",
    "print('feature selection for %s' % e)\n",
    "past_index = [24,25,47,48]\n",
    "features = []\n",
    "targets = []\n",
    "feature_labels=[]\n",
    "for c in df.columns:\n",
    "    for j in past_index:\n",
    "        feature_labels.append(c + '_' + str(j))\n",
    "for pred_hour_index in range(exemplar_prices.shape[0]):\n",
    "    temp_features = []\n",
    "    if type(pred_hour_index) == slice: continue\n",
    "    if pred_hour_index - 48 >= 0:\n",
    "        targets.append(exemplar_prices[pred_hour_index, 0])\n",
    "        for i,c in enumerate(df.columns):\n",
    "            temp_features += [exemplar_prices[pred_hour_index-j, i] for j in past_index]\n",
    "        features.append(temp_features)\n",
    "\n",
    "targets = np.array(targets)\n",
    "features = np.array(features)\n",
    "\n",
    "MIs = []\n",
    "for i in range(features.shape[1]):\n",
    "    MIs.append((feature_labels[i],MI(targets, features[:,i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MIs.sort(key=lambda tup: tup[1], reverse=True) \n",
    "selected = filter(lambda x: x[1]>0.4, MIs)\n",
    "\n",
    "f = open('%s_MIs.pkl' % e,'w+')\n",
    "pickle.dump(selected, f)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f=open('CAPRIDGE_CR1_MIs.pkl','r')\n",
    "MIs = pickle.load(f)\n",
    "f.close()\n",
    "selected = filter(lambda x: x[1]>0.4, MIs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=qcrr.query(exemplars,end_date='2016-5-23')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "targets = []\n",
    "features = []\n",
    "e = exemplars[0]\n",
    "nodes=[]\n",
    "for s in selected:\n",
    "    nodes.append(s[0][:-3])\n",
    "nodes = list(set(nodes))\n",
    "mat=df.as_matrix()\n",
    "for pred_hour_index in range(mat.shape[0]):\n",
    "    temp_features = []\n",
    "    if type(pred_hour_index) == slice: continue\n",
    "    if pred_hour_index - 24 >= 0:\n",
    "        targets.append(mat[pred_hour_index, 0])\n",
    "        for i,c in enumerate(nodes):\n",
    "            temp_features += [mat[pred_hour_index-24, i]]\n",
    "        features.append(temp_features)\n",
    "\n",
    "targets = np.array(targets)\n",
    "features = np.array(features)\n",
    "\n",
    "f=open('features.pkl', 'w+')\n",
    "pickle.dump(features, f)\n",
    "f.close()\n",
    "f=open('targets.pkl', 'w+')\n",
    "pickle.dump(targets, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38419,)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.array(features)\n",
    "targets.shape"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
