{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenleejr92/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:22: DeprecationWarning: the sets module is deprecated\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "import time\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import MySQLdb\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.insert(0, '/Users/kenleejr92/energy_market_project/scripts/MySQL_scripts')\n",
    "from Query_ERCOT_DB import Query_ERCOT_DB\n",
    "import cPickle as pickle\n",
    "from datetime import datetime\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.mllib.linalg import Matrix, Matrices\n",
    "import pyspark.mllib.linalg.distributed as pydist\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.mllib.stat import Statistics\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "import mpld3\n",
    "import re\n",
    "from sets import Set\n",
    "mpld3.enable_notebook()\n",
    "\n",
    "class LMP_Query(Query_ERCOT_DB):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.SPPs = ['HB_BUSAVG', \n",
    "                     'HB_HOUSTON', \n",
    "                     'HB_HUBAVG', \n",
    "                     'HB_NORTH', \n",
    "                     'HB_SOUTH', \n",
    "                     'HB_WEST', \n",
    "                     'LZ_AEN', \n",
    "                     'LZ_CPS', \n",
    "                     'LZ_HOUSTON', \n",
    "                     'LZ_LCRA', \n",
    "                     'LZ_NORTH', \n",
    "                     'LZ_RAYBN', \n",
    "                     'LZ_SOUTH',  \n",
    "                     'LZ_WEST']   \n",
    "        self.node_dict = {}\n",
    "        self.table_list = []\n",
    "        self.df = None\n",
    "        self.CRR_nodes = None\n",
    "        self.table_boundaries = {'table0':('0001', 'BLUEMD1_8X'),\n",
    "                                 'table1':('BLUEMD1_8Z', 'CHT_M'),\n",
    "                                 'table2':('CHT_X', 'DUKE_8405'),\n",
    "                                 'table3':('DUKE_8505', 'ELEVEE_E8'),\n",
    "                                 'table4':('ELEVEE_W8', 'GREENLK_L_A'),\n",
    "                                 'table5':('GREENLK_L_B', 'KEETER'),\n",
    "                                 'table6':('KEITH', 'L_CEDAHI8_1Y'),\n",
    "                                 'table7':('L_CEDAHI8_1Z', 'MOSES_1G'),\n",
    "                                 'table8':('MOSES_2G', 'PHR_8135'),\n",
    "                                 'table9':('PHR_8140', 'SANDOW1_8Y'),\n",
    "                                 'table10':('SANDOW_4G', 'TCN7225_BUS'),\n",
    "                                 'table11': ('TCN7230_BUS', 'VENSW_1777'),\n",
    "                                 'table12':('VENSW_1785', '_WC_V_C')\n",
    "                                 }\n",
    "        for i in range(0,13):\n",
    "            Query_ERCOT_DB.c.execute(\"\"\"SHOW columns FROM DAM_LMP%s\"\"\" % i)\n",
    "            result = [r[0] for r in Query_ERCOT_DB.c.fetchall()[2:]]\n",
    "            self.table_list.append(result)\n",
    "            for node in result:\n",
    "                self.node_dict[node] = i\n",
    "        \n",
    "    \n",
    "    def get_CRR_nodes(self):\n",
    "        Query_ERCOT_DB.c.execute(\"\"\"SELECT DISTINCT Sink FROM crr_ownership ORDER BY Sink\"\"\")\n",
    "        nodes = list(Query_ERCOT_DB.c.fetchall())\n",
    "        CRR_prefixes = [r[0] for r in nodes]\n",
    "        pattern = re.compile('.*_')\n",
    "        matching_patterns = Set()\n",
    "        for idx, node in enumerate(CRR_prefixes):\n",
    "            matches = re.findall(pattern, node)\n",
    "            if matches: matching_patterns.add(matches[0][:-1])\n",
    "        # flatten list of lists      \n",
    "        all_nodes = [item for sublist in self.table_list for item in sublist] + self.SPPs\n",
    "        self.CRR_nodes = []\n",
    "        for pattern in matching_patterns:\n",
    "            pattern2 = re.compile('(.*%s.*)' % pattern)\n",
    "            for node in all_nodes:\n",
    "                if re.search(pattern2, node):\n",
    "                    self.CRR_nodes.append(node)\n",
    "        return self.CRR_nodes\n",
    "    \n",
    "    def query_single_node(self, node):\n",
    "        s=\"\"\"SELECT delivery_date, hour_ending, %s from DAM_LMP%s order by delivery_date, hour_ending\"\"\" % (node, self.node_dict[node])\n",
    "        Query_ERCOT_DB.c.execute(s)\n",
    "        result = list(Query_ERCOT_DB.c.fetchall())\n",
    "        fresult = []\n",
    "        for r in result:\n",
    "            temp = ()\n",
    "            date = r[0]\n",
    "            time = str(int(r[1].split(\":\")[0])-1)\n",
    "            dt = datetime.strptime(date + \" \" + time, \"%Y-%m-%d %H\")\n",
    "            for x in r[2:]:\n",
    "                if x == None: x = 0\n",
    "                temp = temp + (float(x),)\n",
    "            r = (dt,) + temp\n",
    "            fresult.append(r)\n",
    "        self.df = pd.DataFrame(data=[f[1:] for f in fresult], index=[r[0] for r in fresult], columns=[node])\n",
    "        \n",
    "    def query(self, nodes, start_date='2011-01-01', end_date='2015-12-31'):\n",
    "        node_string=''\n",
    "        for node in nodes:\n",
    "            node_string = node_string + node + ',' + ' '\n",
    "        node_string = node_string[:-2]\n",
    "        s = \"\"\"SELECT DAM_LMP0.delivery_date, DAM_LMP0.hour_ending, %s \n",
    "                                    from DAM_LMP0 join DAM_LMP1 on (DAM_LMP0.delivery_date = DAM_LMP1.delivery_date and DAM_LMP0.hour_ending = DAM_LMP1.hour_ending) \n",
    "                                    join DAM_LMP2 on (DAM_LMP0.delivery_date = DAM_LMP2.delivery_date and DAM_LMP0.hour_ending = DAM_LMP2.hour_ending)\n",
    "                                    join DAM_LMP3 on (DAM_LMP0.delivery_date = DAM_LMP3.delivery_date and DAM_LMP0.hour_ending = DAM_LMP3.hour_ending)\n",
    "                                    join DAM_LMP4 on (DAM_LMP0.delivery_date = DAM_LMP4.delivery_date and DAM_LMP0.hour_ending = DAM_LMP4.hour_ending)\n",
    "                                    join DAM_LMP5 on (DAM_LMP0.delivery_date = DAM_LMP5.delivery_date and DAM_LMP0.hour_ending = DAM_LMP5.hour_ending)\n",
    "                                    join DAM_LMP6 on (DAM_LMP0.delivery_date = DAM_LMP6.delivery_date and DAM_LMP0.hour_ending = DAM_LMP6.hour_ending)\n",
    "                                    join DAM_LMP7 on (DAM_LMP0.delivery_date = DAM_LMP7.delivery_date and DAM_LMP0.hour_ending = DAM_LMP7.hour_ending)\n",
    "                                    join DAM_LMP8 on (DAM_LMP0.delivery_date = DAM_LMP8.delivery_date and DAM_LMP0.hour_ending = DAM_LMP8.hour_ending)\n",
    "                                    join DAM_LMP9 on (DAM_LMP0.delivery_date = DAM_LMP9.delivery_date and DAM_LMP0.hour_ending = DAM_LMP9.hour_ending)\n",
    "                                    join DAM_LMP10 on (DAM_LMP0.delivery_date = DAM_LMP10.delivery_date and DAM_LMP0.hour_ending = DAM_LMP10.hour_ending)\n",
    "                                    join DAM_LMP11 on (DAM_LMP0.delivery_date = DAM_LMP11.delivery_date and DAM_LMP0.hour_ending = DAM_LMP11.hour_ending)\n",
    "                                    join DAM_LMP12 on (DAM_LMP0.delivery_date = DAM_LMP12.delivery_date and DAM_LMP0.hour_ending = DAM_LMP12.hour_ending)\n",
    "                                    join DAM_SPPs on (DAM_LMP0.delivery_date = DAM_SPPs.delivery_date and DAM_LMP0.hour_ending = DAM_SPPs.hour_ending)\n",
    "                                    where DAM_LMP0.delivery_date > \"%s\" and DAM_LMP0.delivery_date < \"%s\" order by DAM_LMP0.delivery_date, DAM_LMP0.hour_ending;\"\"\" % (node_string, start_date, end_date)\n",
    "        Query_ERCOT_DB.c.execute(s)\n",
    "        result = list(Query_ERCOT_DB.c.fetchall())\n",
    "        fresult = []\n",
    "        for r in result:\n",
    "            temp = ()\n",
    "            date = r[0]\n",
    "            time = str(int(r[1].split(\":\")[0])-1)\n",
    "            dt = datetime.strptime(date + \" \" + time, \"%Y-%m-%d %H\")\n",
    "            for x in r[2:]:\n",
    "                if x == None: x = 0\n",
    "                temp = temp + (float(x),)\n",
    "            r = (dt,) + temp\n",
    "            fresult.append(r)\n",
    "        self.df = pd.DataFrame(data=[f[1:] for f in fresult], index=[r[0] for r in fresult], columns=nodes)\n",
    "        return self.df\n",
    "        \n",
    "    def get_price(self, node, date, hour_ending):\n",
    "        for i in range(0,13):\n",
    "            node = append_n(node)\n",
    "            if node in self.table_columns['table%s' % i]:\n",
    "                Query_ERCOT_DB.c.execute(\"\"\"SELECT %s FROM DAM_LMP%s WHERE delivery_date = \"%s\" AND hour_ending = \\\"%s\\\"\"\"\" % (node, i, date, hour_ending))\n",
    "                result = list(Query_ERCOT_DB.c.fetchall())[0][0]\n",
    "                return result\n",
    "        \n",
    "\n",
    "def append_n(name):\n",
    "    if name[0] in ['0','1','2','3','4','5','6','7','8','9'] or name == 'LOAD':\n",
    "        name = 'n' + name\n",
    "    return name\n",
    "\n",
    "def dist(x,y):\n",
    "    cost = 0\n",
    "    for i in range(len(x)):\n",
    "        if x[i] == 0 or y[i] == 0:\n",
    "            continue\n",
    "        else:\n",
    "            cost = cost + np.abs(y[i] - x[i])[0]\n",
    "    return cost\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conf = SparkConf().setMaster(\"local\").setAppName(\"spark_DAM_correlation\")\n",
    "sc = SparkContext(conf = conf)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "qcrr = LMP_Query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crr_nodes = qcrr.get_CRR_nodes()\n",
    "f = open('crr_nodes.pkl', 'w+')\n",
    "pickle.dump(crr_nodes, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1111)\n",
    "f2 = open('crr_nodes.pkl', 'r')\n",
    "crr_nodes = pickle.load(f2)\n",
    "f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x1 = qcrr.query(crr_nodes[0:500],'2011-01-01','2015-12-31').as_matrix()\n",
    "x2 = qcrr.query(crr_nodes[500:1000], '2011-01-01','2015-12-31').as_matrix()\n",
    "x3 = qcrr.query(crr_nodes[1000:1500], '2011-01-01','2015-12-31').as_matrix()\n",
    "x4 = qcrr.query(crr_nodes[1500:2000], '2011-01-01','2015-12-31').as_matrix()\n",
    "x5 = qcrr.query(crr_nodes[2000:2500], '2011-01-01','2015-12-31').as_matrix()\n",
    "x6 = qcrr.query(crr_nodes[2500:3000], '2011-01-01','2015-12-31').as_matrix()\n",
    "x7 = qcrr.query(crr_nodes[3000:], '2011-01-01','2015-12-31').as_matrix()\n",
    "x8 = np.concatenate((x1,x2), axis=1)\n",
    "x8 = np.concatenate((x8,x3), axis=1)\n",
    "x8 = np.concatenate((x8,x4), axis=1)\n",
    "x8 = np.concatenate((x8,x5), axis=1)\n",
    "x8 = np.concatenate((x8,x6), axis=1)\n",
    "x8 = np.concatenate((x8,x7), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1 = open('crr_node_prices.csv', 'w+')\n",
    "np.savetxt('crr_node_prices.csv', x8, delimiter=',', fmt='%10.5f')\n",
    "f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_df = pd.DataFrame.from_csv('/Users/kenleejr92/Downloads/part-0000', header=None, index_col=None)\n",
    "x = x_df.as_matrix()\n",
    "for i, item in enumerate(x[:,0]):\n",
    "    x[i,0] = float(item[1:])\n",
    "for i, item in enumerate(x[:,3587]):\n",
    "    x[i,3587] = float(item[:-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "correlation_matrix = np.abs(x)\n",
    "af = AffinityPropagation(affinity='precomputed').fit(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  29,   34,   57,   61,   62,   64,   72,   87,   92,  190,  192,\n",
       "        208,  210,  213,  216,  222,  259,  280,  290,  297,  326,  327,\n",
       "        335,  344,  348,  356,  365,  392,  404,  407,  408,  431,  446,\n",
       "        492,  495,  535,  550,  594,  601,  602,  607,  612,  626,  631,\n",
       "        633,  635,  637,  651,  659,  663,  669,  674,  683,  690,  747,\n",
       "        768,  773,  808,  829,  831,  849,  878,  895,  896,  897,  900,\n",
       "        902,  903,  904,  905,  906,  908,  909,  911,  927,  944,  966,\n",
       "        971,  979,  990,  991,  997,  998, 1037, 1082, 1091, 1119, 1156,\n",
       "       1159, 1160, 1186, 1263, 1264, 1265, 1300, 1329, 1345, 1358, 1361,\n",
       "       1366, 1375, 1407, 1411, 1445, 1452, 1453, 1454, 1457, 1458, 1459,\n",
       "       1460, 1461, 1462, 1481, 1484, 1505, 1523, 1546, 1555, 1560, 1571,\n",
       "       1576, 1592, 1608, 1615, 1620, 1680, 1682, 1691, 1716, 1727, 1732,\n",
       "       1733, 1736, 1737, 1738, 1739, 1740, 1741, 1757, 1777, 1789, 1798,\n",
       "       1816, 1833, 1834, 1835, 1836, 1839, 1844, 1850, 1889, 1982, 2035,\n",
       "       2046, 2048, 2049, 2055, 2060, 2063, 2067, 2089, 2136, 2137, 2140,\n",
       "       2151, 2152, 2159, 2175, 2176, 2177, 2178, 2179, 2180, 2181, 2182,\n",
       "       2183, 2213, 2220, 2255, 2256, 2300, 2307, 2338, 2341, 2371, 2377,\n",
       "       2378, 2392, 2394, 2395, 2422, 2440, 2443, 2444, 2448, 2451, 2453,\n",
       "       2454, 2463, 2471, 2473, 2477, 2483, 2484, 2500, 2501, 2520, 2521,\n",
       "       2528, 2553, 2554, 2564, 2568, 2580, 2584, 2592, 2603, 2604, 2622,\n",
       "       2649, 2654, 2670, 2690, 2693, 2725, 2748, 2798, 2817, 2832, 2833,\n",
       "       2834, 2847, 2854, 2864, 2900, 2946, 2949, 2986, 3032, 3045, 3051,\n",
       "       3056, 3057, 3071, 3072, 3073, 3075, 3087, 3089, 3106, 3107, 3115,\n",
       "       3125, 3127, 3128, 3129, 3130, 3131, 3132, 3133, 3134, 3138, 3152,\n",
       "       3157, 3165, 3170, 3200, 3216, 3217, 3231, 3247, 3273, 3282, 3293,\n",
       "       3294, 3301, 3309, 3314, 3318, 3320, 3322, 3329, 3331, 3332, 3335,\n",
       "       3353, 3354, 3366, 3372, 3375, 3376, 3384, 3395, 3397, 3404, 3406,\n",
       "       3418, 3425, 3438, 3502, 3528, 3535, 3545, 3546, 3547, 3548, 3549,\n",
       "       3550, 3551, 3552, 3553, 3554, 3555, 3575])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_dict = {}\n",
    "for i, col in enumerate(crr_nodes):\n",
    "    col_dict[i] = col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cluster_list = [[] for i in range(af.cluster_centers_indices_.size)]\n",
    "for idx, item in enumerate(af.labels_):\n",
    "    cluster_list[item].append(col_dict[idx])\n",
    "exemplars = []\n",
    "for idx, cluster in enumerate(cluster_list):\n",
    "    if len(cluster)>10:\n",
    "        exemplars.append(col_dict[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = qcrr.query(exemplars)\n",
    "ax = df.plot()\n",
    "ax.legend_.remove()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "for i in df.columns:\n",
    "    x = df[i]\n",
    "    obj = seasonal_decompose(df[i], freq=60)\n",
    "    no_hourly = x - obj.seasonal\n",
    "    obj2 = seasonal_decompose(no_hourly, freq=24*7)\n",
    "    no_weekly = no_hourly - obj2.seasonal\n",
    "    plt.plot(no_weekly)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-e5e911fd6c03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcongestion_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mcongestion_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers, not tuple"
     ]
    }
   ],
   "source": [
    "exemplar_matrix = df.as_matrix()\n",
    "n = len(exemplars)\n",
    "congestion_matrix = []\n",
    "for row in exemplar_matrix:\n",
    "    x = np.zeros((n,n))\n",
    "    for idx,i in enumerate(row):\n",
    "        for idx2,j in enumerate(row):\n",
    "            x[idx,idx2] = i - j\n",
    "    congestion_matrix.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.fftpack import fft\n",
    "# Number of sample points\n",
    "y = df.as_matrix()\n",
    "N = y.shape[0]\n",
    "# sample spacing\n",
    "T = 3600.0\n",
    "x = np.linspace(0.0, N*T, N)\n",
    "yf = np.fft.fftn(y)\n",
    "xf = np.linspace(0.0, 1.0/(2.0*T), N/2)\n",
    "plt.plot(xf, 2.0/N * np.abs(yf[0:N/2]))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for cluster_num in range(len(cluster_list)):\n",
    "    cluster_nodes = cluster_list[cluster_num]\n",
    "    if len(cluster_nodes) >= 4:\n",
    "        cluster = qcrr.df[cluster_nodes]\n",
    "        ax = cluster.plot()\n",
    "        ax.legend()\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
