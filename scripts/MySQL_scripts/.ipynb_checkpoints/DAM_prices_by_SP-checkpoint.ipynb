{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import MySQLdb\n",
    "from MySQLdb.constants import FIELD_TYPE\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import holidays\n",
    "import random\n",
    "from sklearn import cross_validation\n",
    "from sklearn import datasets\n",
    "from sklearn import preprocessing\n",
    "from Query_ERCOT_DB import Query_ERCOT_DB\n",
    "\n",
    "MONTHS_PER_YEAR = 12\n",
    "DAYS_PER_MONTH = 30\n",
    "HRS_PER_DAY = 24\n",
    "\n",
    "# Acquire DAM SPP for all settlement points for a specific date range\n",
    "class Feature_Processor(Query_ERCOT_DB):\n",
    "    # list of settlement points is common across all instances of the Feature_Processor class\n",
    "    table_headers = []\n",
    "    Query_ERCOT_DB.c.execute(\"\"\"SHOW COLUMNS FROM DAM_SPPs\"\"\")\n",
    "    r = list(Query_ERCOT_DB.c.fetchall())\n",
    "    for sp in r:\n",
    "        if sp[0] == \"delivery_date\" or sp[0] == \"hour_ending\":\n",
    "            continue\n",
    "        table_headers.append(sp[0])\n",
    "    Query_ERCOT_DB.c.execute(\"\"\"SHOW COLUMNS FROM Load_by_LZ\"\"\")\n",
    "    r = list(Query_ERCOT_DB.c.fetchall())\n",
    "    for sp in r:\n",
    "        if sp[0] == \"delivery_date\" or sp[0] == \"hour_ending\":\n",
    "            continue\n",
    "        table_headers.append(sp[0])\n",
    "    '''\n",
    "    Query the list of settlement points and remove the heading \"Settlement Point\"\n",
    "    self.start_date - start date of query\n",
    "    self.end_date - end date of query\n",
    "    self.dts - list of date_time objects in the query result\n",
    "    self.df - pandas data frame representing the query result\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.start_date = None\n",
    "        self.end_date = None\n",
    "        self.dts = None\n",
    "        self.df = None\n",
    "        self.features_df = None\n",
    "        self.output_norm = None\n",
    "        self.output_mean = None\n",
    "        self.output_std = None\n",
    "        self.train_df = None\n",
    "        self.val_df = None\n",
    "        self.test_df = None\n",
    "\n",
    "    '''\n",
    "    Query for all prices for all load zones and hubs for specified date range\n",
    "    Creates a pandas data frame of the query\n",
    "    '''\n",
    "    def query(self, sd, ed):\n",
    "        self.start_date = sd\n",
    "        self.end_date = ed\n",
    "        Query_ERCOT_DB.c.execute(\"\"\"SELECT * FROM DAM_SPPs\n",
    "                INNER JOIN Load_by_LZ\n",
    "                USING (delivery_date,hour_ending)\n",
    "                WHERE DAM_SPPs.delivery_date > \"%s\"\n",
    "                AND DAM_SPPs.delivery_date < \"%s\"\n",
    "                ORDER BY DAM_SPPs.delivery_date, DAM_SPPs.hour_ending\"\"\" % (sd, ed))\n",
    "        result = list(Query_ERCOT_DB.c.fetchall())\n",
    "        fresult = []\n",
    "        for r in result:\n",
    "            temp = ()\n",
    "            date = r[0]\n",
    "            time = str(int(r[1].split(\":\")[0])-1)\n",
    "            dt = datetime.strptime(date + \" \" + time, \"%Y-%m-%d %H\")\n",
    "            for x in r[2:]:\n",
    "                temp = temp + (float(x),)\n",
    "            r = (dt,) + temp\n",
    "            fresult.append(r)\n",
    "        self.df = pd.DataFrame(data=[f[1:] for f in fresult], index=[r[0] for r in fresult], columns=self.table_headers)\n",
    "        self.dts = self.df.index\n",
    "\n",
    "    '''\n",
    "    Given a load zone or hub, creates a feature data frame for the specified model\n",
    "    Model A (Benchmark):\n",
    "        Input1: Day-Type indicator\n",
    "        Input2: Hour indicator\n",
    "        Input3: Holiday indicator\n",
    "        Input4: Hourly price of day d-1\n",
    "        Input5: Hourly price of day d-7\n",
    "    Model B (exogenous variables):\n",
    "    '''\n",
    "    def construct_feature_vector_matrix(self, lzhub, model_type, normalization = 'L2_norm'):\n",
    "        dflzhub = self.df[lzhub + '_SPP']\n",
    "        load_df = self.df[lzhub + '_load']\n",
    "        features = []\n",
    "        feature_labels = None\n",
    "        numerical_features = None\n",
    "        idx_wout_1st_week = None\n",
    "        \n",
    "        if model_type == \"A\":\n",
    "            for dt, price in dflzhub.iteritems():\n",
    "                pred_hour_index = dflzhub.index.get_loc(dt)\n",
    "                if pred_hour_index - 7*24 >= 0:\n",
    "                    features.append([work_day_or_holiday(dt),\n",
    "                                          dt.hour,\n",
    "                                          dt.weekday(),\n",
    "                                          dt.month,\n",
    "                                          dflzhub.iloc[pred_hour_index - 1*24],\n",
    "                                          dflzhub.iloc[pred_hour_index - 7*24]])\n",
    "            feature_labels = ['Holiday', 'Hour', 'Day', 'Month', 'P(h-24)', 'P(h-168)']\n",
    "            idx_wout_1st_week = list(dflzhub.index.values)[7*24:]\n",
    "\n",
    "        if model_type == 'B':\n",
    "            for dt, price in dflzhub.iteritems():\n",
    "                pred_hour_index = dflzhub.index.get_loc(dt)\n",
    "                if pred_hour_index - 7*24 >= 0:\n",
    "                    features.append([work_day_or_holiday(dt),\n",
    "                                          dt.hour,\n",
    "                                          dt.weekday(),\n",
    "                                          dt.month,\n",
    "                                          dflzhub.iloc[pred_hour_index - 24],\n",
    "                                          dflzhub.iloc[pred_hour_index - 25],\n",
    "                                          dflzhub.iloc[pred_hour_index - 47],\n",
    "                                          dflzhub.iloc[pred_hour_index - 48],\n",
    "                                          dflzhub.iloc[pred_hour_index - 72],\n",
    "                                          dflzhub.iloc[pred_hour_index - 120],\n",
    "                                          dflzhub.iloc[pred_hour_index - 144],\n",
    "                                          dflzhub.iloc[pred_hour_index - 167],\n",
    "                                          dflzhub.iloc[pred_hour_index - 168]])\n",
    "            feature_labels = ['Holiday', 'Hour', 'Day', 'Month', 'P(h-24)', 'P(h-25)', 'P(h-47)', 'P(h-48)', 'P(h-72)',\\\n",
    "                              'P(h-120)', 'P(h-144)', 'P(h-167)', 'P(h-168)']\n",
    "            idx_wout_1st_week = list(dflzhub.index.values)[7*24:]\n",
    "\n",
    "        if model_type == 'C':\n",
    "            for dt, price in dflzhub.iteritems():\n",
    "                pred_hour_index = dflzhub.index.get_loc(dt)\n",
    "                if pred_hour_index - 7*24 >= 0:\n",
    "                    features.append([work_day_or_holiday(dt),\n",
    "                                          dt.hour,\n",
    "                                          dt.weekday(),\n",
    "                                          dt.month,\n",
    "                                          load_df.iloc[pred_hour_index],\n",
    "                                          dflzhub.iloc[pred_hour_index - 1*24],\n",
    "                                          dflzhub.iloc[pred_hour_index - 7*24],\n",
    "                                          load_df.iloc[pred_hour_index - 1*24],\n",
    "                                          load_df.iloc[pred_hour_index - 7*24]])\n",
    "            feature_labels = ['Holiday', 'Hour', 'Day', 'Month', 'FLoad', 'P(h-24)', 'P(h-168)', 'L(h-24)', 'L(h-168)']\n",
    "            idx_wout_1st_week = list(dflzhub.index.values)[7*24:]\n",
    "        # features dataframe before one-hot encoding and normalization of numerical features\n",
    "        self.features_df = pd.DataFrame(data=features,\n",
    "                                   index=idx_wout_1st_week,\n",
    "                                   columns=feature_labels)\n",
    "    \n",
    "        self.features_df = encode_onehot(self.features_df, 'Day')\n",
    "        self.features_df = encode_onehot(self.features_df, 'Month')\n",
    "        self.features_df = encode_onehot(self.features_df, 'Hour')\n",
    "        self.features_df = self.features_df.join(dflzhub, how='left')\n",
    "        return self.features_df\n",
    "    \n",
    "\n",
    "        \n",
    "    \n",
    "    def scale_num_features(self, lzhub, model, df, method):\n",
    "        numerical_features = []\n",
    "        if model == 'A':\n",
    "            numerical_features = ['P(h-24)', 'P(h-168)']\n",
    "        if model == 'B':\n",
    "            numerical_features = ['P(h-24)', 'P(h-25)', 'P(h-47)', 'P(h-48)', 'P(h-72)',\\\n",
    "                              'P(h-120)', 'P(h-144)', 'P(h-167)', 'P(h-168)']\n",
    "        if model == 'C':\n",
    "            numerical_features = ['FLoad', 'P(h-24)', 'P(h-168)', 'L(h-24)', 'L(h-168)']\n",
    "        numerical_features.append(lzhub + '_SPP')\n",
    "        \n",
    "            \n",
    "        if method == 'L2_norm':\n",
    "            output_norm = np.linalg.norm(df[lzhub + '_SPP'].as_matrix())\n",
    "            df = normalize(df, numerical_features)\n",
    "            return df, output_norm\n",
    "        \n",
    "        if method == 'standard_scale':\n",
    "            output_mean = np.mean(self.df[lzhub + '_SPP'].as_matrix())\n",
    "            output_std = np.std(self.df[lzhub + '_SPP'].as_matrix())\n",
    "            df = standard_scale(df, numerical_features)\n",
    "            return df, output_mean, output_std\n",
    "        \n",
    "        if method == 'robust_scale':\n",
    "            df = robust_scale(df, numerical_features)\n",
    "        if method == 'max_abs_scale':\n",
    "            df = max_abs_scale(df, numerical_features)\n",
    "            \n",
    "        return \n",
    "    \n",
    "    def rescale(self, y, method, output_norm=0, output_std=0, output_mean=0):\n",
    "        if method == 'L2_norm':\n",
    "            return y*output_norm\n",
    "        if method == 'standard_scale':\n",
    "            return y*output_std + output_mean\n",
    "   \n",
    "    '''\n",
    "    Plots the prices for all load zones and hubs for the specified date range\n",
    "    '''\n",
    "    def plot(self):\n",
    "        self.df.plot()\n",
    "        plt.title(\"SPP by LZ and HUB for %s\" % self.start_date.split(\"-\")[0])\n",
    "        plt.xlabel(\"Date-Time\")\n",
    "        plt.ylabel(\"SPP\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def get_settlement_points(self):\n",
    "        return self.table_headers\n",
    "\n",
    "    '''\n",
    "    Splits the feature data frame into train, test, and validation sets\n",
    "    Performs seasonal sampling; splits the date range into months and then samples within each month without replacement\n",
    "        60% of each month for training\n",
    "        20% of each month for validation\n",
    "        20% of each month for testing\n",
    "    '''\n",
    "    def train_test_validate(self, train_size=0.6, test_size=0.2):\n",
    "        ft = self.features_df\n",
    "        train_indices = []\n",
    "        test_indices = []\n",
    "        val_indices = []\n",
    "        for i in range(MONTHS_PER_YEAR):\n",
    "            train_i, test_i, val_i = sample_month(i, train_size, test_size)\n",
    "            train_indices = train_indices + train_i\n",
    "            test_indices = test_indices + test_i\n",
    "            val_indices = val_indices + val_i\n",
    "        train_indices = [i*HRS_PER_DAY for i in train_indices]\n",
    "        test_indices = [i*HRS_PER_DAY for i in test_indices]\n",
    "        val_indices = [i*HRS_PER_DAY for i in val_indices]\n",
    "        train_dfs = []\n",
    "        test_dfs = []\n",
    "        val_dfs = []\n",
    "        for i in train_indices:\n",
    "            train_dfs.append(ft.iloc[i:i+HRS_PER_DAY])\n",
    "        self.train_df = pd.concat(train_dfs)\n",
    "        for i in test_indices:\n",
    "            test_dfs.append(ft.iloc[i:i+HRS_PER_DAY])\n",
    "        self.test_df = pd.concat(test_dfs)\n",
    "        for i in val_indices:\n",
    "            val_dfs.append(ft.iloc[i:i+HRS_PER_DAY])\n",
    "        self.val_df = pd.concat(val_dfs)\n",
    "        return self.train_df, self.val_df, self.test_df\n",
    "    \n",
    "    def convert_dfs_to_numpy(self,df):\n",
    "        num_features = df.as_matrix().shape[1]\n",
    "        return (df.ix[:, 0:num_features-1].as_matrix(), df.ix[:, num_features-1].as_matrix()) \n",
    "        \n",
    "def string_to_date(string_date):\n",
    "    return datetime.strptime(string_date, \"%Y-%m-%d %H\")\n",
    "\n",
    "def date_to_string(date):\n",
    "    return date.strftime(\"%Y-%m-%d %H\")\n",
    "\n",
    "def weekday_of_date(date):\n",
    "    return calendar.day_name[date.weekday()]\n",
    "\n",
    "def work_day_or_holiday(date):\n",
    "    us_holidays = holidays.UnitedStates()\n",
    "    if date in us_holidays or weekday_of_date(date) == \"Sunday\" or weekday_of_date(date) == \"Saturday\":\n",
    "        return int(1)\n",
    "    else: return int(0)\n",
    "    \n",
    "def encode_onehot(df, cols):\n",
    "    enc = preprocessing.OneHotEncoder()\n",
    "    index = df[cols].index\n",
    "    data = enc.fit_transform(df[cols].reshape(-1, 1)).toarray()\n",
    "    one_hot_df = pd.DataFrame(data=data, index=index, columns=[cols + '%s' % i for i in range(data.shape[1])])\n",
    "    del df[cols]\n",
    "    return df.join(one_hot_df, how='inner')\n",
    "\n",
    "def max_abs_scale(df,cols):\n",
    "    max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "    df[cols] = max_abs_scaler.fit_transform(df[cols])\n",
    "    return df\n",
    "\n",
    "def min_max_scale(df,cols):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    df[cols] = min_max_scaler.fit_transform(df[cols])\n",
    "    return df\n",
    "\n",
    "def standard_scale(df,cols):\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    df[cols] = scaler.fit_transform(df[cols])\n",
    "    return df\n",
    "\n",
    "def robust_scale(df,cols):\n",
    "    rscaler = preprocessing.RobustScaler()\n",
    "    df[cols] = rscaler.fit_transform(df[cols])\n",
    "    return df\n",
    "\n",
    "def normalize(df,cols):\n",
    "    normalizer = preprocessing.Normalizer()\n",
    "    df[cols] = normalizer.fit_transform(df[cols])\n",
    "    return df\n",
    "\n",
    "def sample_month(month_index, train_size, test_size):\n",
    "    np.random.seed(22943)\n",
    "    indices = np.arange(0, DAYS_PER_MONTH)\n",
    "    set_indices = set(indices)\n",
    "    train_indices = np.random.choice(indices,\n",
    "                                  int(DAYS_PER_MONTH*train_size),\n",
    "                                  replace=False).tolist()\n",
    "    test_indices = np.random.choice(list(set_indices.difference(set(train_indices))),\n",
    "                                 int(DAYS_PER_MONTH*test_size),\n",
    "                                 replace=False).tolist()\n",
    "    val_indices = list(set_indices.difference(set(train_indices)).difference(test_indices))\n",
    "\n",
    "    train_indices = [i + month_index*DAYS_PER_MONTH for i in train_indices]\n",
    "    test_indices = [i + month_index*DAYS_PER_MONTH for i in test_indices]\n",
    "    val_indices = [i + month_index*DAYS_PER_MONTH for i in val_indices]\n",
    "    return train_indices, test_indices, val_indices\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lzhub = 'LZ_NORTH'\n",
    "model = 'A'\n",
    "fp = Feature_Processor()\n",
    "fp.query('2012-01-01', '2012-12-31')\n",
    "feature_df = fp.construct_feature_vector_matrix(lzhub, model, 'standard_scale')\n",
    "fp.train_test_validate()\n",
    "norm_train_set, output_mean, output_std = fp.scale_num_features(lzhub, model, fp.train_df, 'standard_scale')\n",
    "norm_test_set, output_testmean, output_teststd = fp.scale_num_features(lzhub, model, fp.test_df, 'standard_scale')\n",
    "final_train_set = fp.convert_dfs_to_numpy(norm_train_set)\n",
    "final_test_set = fp.convert_dfs_to_numpy(norm_test_set)\n",
    "x_train, y_train = final_train_set\n",
    "x_test, y_test = final_test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
